{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/peterchang0414/lecun1989-flax/blob/main/lecun1989-flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVrzk2Bvacs-"
   },
   "source": [
    "# Backpropagation Applied to MNIST\n",
    "Based on Lecun 1989: http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\n",
    "\n",
    "Adapted to JAX from https://github.com/karpathy/lecun1989-repro/blob/master/prepro.py\n",
    "\n",
    "Author: Peter G. Chang ([@peterchang0414](https://github.com/peterchang0414))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7JVDrbphkxe"
   },
   "source": [
    "# 1989 Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe2o-uZZd4yT"
   },
   "outputs": [],
   "source": [
    "!pip install -q flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pc7McMNRnt2E"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    from flax import linen as nn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax import linen as nn\n",
    "try:\n",
    "    from torchvision import datasets\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq torchvision\n",
    "    from torchvision import datasets\n",
    "\n",
    "\n",
    "def get_datasets(n_tr, n_te):\n",
    "    train_test = {}\n",
    "    for split in {\"train\", \"test\"}:\n",
    "        data = datasets.MNIST(\"./data\", train=split == \"train\", download=True)\n",
    "        n = n_tr if split == \"train\" else n_te\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        rp = jax.random.permutation(key, len(data))[:n]\n",
    "        X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
    "        Y = jnp.full((n, 10), -1.0, dtype=jnp.float32)\n",
    "        for i, ix in enumerate(rp):\n",
    "            I, yint = data[int(ix)]\n",
    "            xi = jnp.array(I, dtype=jnp.float32) / 127.5 - 1.0\n",
    "            xi = jax.image.resize(xi, (16, 16), \"bilinear\")\n",
    "            X = X.at[i].set(jnp.expand_dims(xi, axis=2))\n",
    "            Y = Y.at[i, yint].set(1.0)\n",
    "        train_test[split] = (X, Y)\n",
    "    return train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk3obOlvqoDZ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from flax import linen as nn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.linen.activation import tanh\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq optax\n",
    "    import optax\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "    # sqrt(6) = 2.449... used by he_uniform() approximates Karpathy's 2.4\n",
    "    kernel_init: Callable = nn.initializers.he_uniform()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = jnp.pad(x, [(0, 0), (2, 2), (2, 2), (0, 0)], constant_values=-1.0)\n",
    "        x = nn.Conv(\n",
    "            features=12, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        bias1 = self.param(\"bias1\", self.bias_init, (8, 8, 12))\n",
    "        x = tanh(x + bias1)\n",
    "        x = jnp.pad(x, [(0, 0), (2, 2), (2, 2), (0, 0)], constant_values=-1.0)\n",
    "        x1, x2, x3 = (x[..., 0:8], x[..., 4:12], jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
    "        slice1 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x1)\n",
    "        slice2 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x2)\n",
    "        slice3 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x3)\n",
    "        x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
    "        bias2 = self.param(\"bias2\", self.bias_init, (4, 4, 12))\n",
    "        x = tanh(x + bias2)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=30, use_bias=False)(x)\n",
    "        bias3 = self.param(\"bias3\", self.bias_init, (30,))\n",
    "        x = tanh(x + bias3)\n",
    "        x = nn.Dense(features=10, use_bias=False)(x)\n",
    "        bias4 = self.param(\"bias4\", nn.initializers.constant(-1.0), (10,))\n",
    "        x = tanh(x + bias4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUCi-jvDXuha"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, X, Y):\n",
    "    Yhat = Net().apply({\"params\": params}, X)\n",
    "    loss = jnp.mean((Yhat - Y) ** 2)\n",
    "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
    "    return loss, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9KDZtpgVUBu"
   },
   "outputs": [],
   "source": [
    "def eval_split(data, split, params):\n",
    "    X, Y = data[split]\n",
    "    loss, err = eval_step(params, X, Y)\n",
    "    print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwzevZv3CdvA"
   },
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq optax\n",
    "    import optax\n",
    "try:\n",
    "    from flax.training import train_state\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax.training import train_state\n",
    "\n",
    "\n",
    "def create_train_state(key, lr, X):\n",
    "    model = Net()\n",
    "    params = model.init(key, X)[\"params\"]\n",
    "    sgd_opt = optax.sgd(lr)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=sgd_opt)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, X, Y):\n",
    "    def loss_fn(params):\n",
    "        Yhat = Net().apply({\"params\": params}, X)\n",
    "        loss = jnp.mean((Yhat - Y) ** 2)\n",
    "        err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
    "        return loss, err\n",
    "\n",
    "    (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "\n",
    "def train_one_epoch(state, X, Y):\n",
    "    for step_num in range(X.shape[0]):\n",
    "        x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
    "        state = train_step(state, x, y)\n",
    "    return state\n",
    "\n",
    "\n",
    "def train(key, data, epochs, lr):\n",
    "    Xtr, Ytr = data[\"train\"]\n",
    "    Xte, Yte = data[\"test\"]\n",
    "    train_state = create_train_state(key, lr, Xtr)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch+1}\")\n",
    "        train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            eval_split(data, split, train_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phiwVJtuFerc"
   },
   "outputs": [],
   "source": [
    "data = get_datasets(7291, 2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZhSr9-YKlo5",
    "outputId": "56fac0fa-e157-4eab-a5ec-b98846758252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "eval: split train. loss 5.576071e-02. error 8.11%. misses: 591\n",
      "eval: split test . loss 5.287848e-02. error 7.37%. misses: 148\n",
      "epoch 2\n",
      "eval: split train. loss 4.097378e-02. error 5.80%. misses: 423\n",
      "eval: split test . loss 4.257497e-02. error 6.08%. misses: 122\n",
      "epoch 3\n",
      "eval: split train. loss 3.390130e-02. error 4.92%. misses: 359\n",
      "eval: split test . loss 3.796291e-02. error 5.48%. misses: 110\n",
      "epoch 4\n",
      "eval: split train. loss 2.989994e-02. error 4.38%. misses: 319\n",
      "eval: split test . loss 3.480190e-02. error 5.23%. misses: 105\n",
      "epoch 5\n",
      "eval: split train. loss 2.566473e-02. error 3.77%. misses: 275\n",
      "eval: split test . loss 3.232093e-02. error 4.73%. misses: 95\n",
      "epoch 6\n",
      "eval: split train. loss 2.348944e-02. error 3.33%. misses: 242\n",
      "eval: split test . loss 3.208887e-02. error 4.58%. misses: 92\n",
      "epoch 7\n",
      "eval: split train. loss 2.151174e-02. error 3.09%. misses: 225\n",
      "eval: split test . loss 3.206819e-02. error 4.93%. misses: 99\n",
      "epoch 8\n",
      "eval: split train. loss 1.941714e-02. error 2.77%. misses: 202\n",
      "eval: split test . loss 3.061979e-02. error 4.73%. misses: 95\n",
      "epoch 9\n",
      "eval: split train. loss 1.694829e-02. error 2.41%. misses: 176\n",
      "eval: split test . loss 2.916610e-02. error 4.38%. misses: 88\n",
      "epoch 10\n",
      "eval: split train. loss 1.605429e-02. error 2.22%. misses: 162\n",
      "eval: split test . loss 2.967581e-02. error 4.58%. misses: 92\n",
      "epoch 11\n",
      "eval: split train. loss 1.565071e-02. error 2.18%. misses: 159\n",
      "eval: split test . loss 3.011220e-02. error 4.58%. misses: 92\n",
      "epoch 12\n",
      "eval: split train. loss 1.397184e-02. error 1.93%. misses: 141\n",
      "eval: split test . loss 2.919692e-02. error 4.53%. misses: 91\n",
      "epoch 13\n",
      "eval: split train. loss 1.240323e-02. error 1.59%. misses: 116\n",
      "eval: split test . loss 2.727516e-02. error 3.64%. misses: 73\n",
      "epoch 14\n",
      "eval: split train. loss 1.198561e-02. error 1.56%. misses: 114\n",
      "eval: split test . loss 2.697299e-02. error 3.89%. misses: 78\n",
      "epoch 15\n",
      "eval: split train. loss 1.133908e-02. error 1.44%. misses: 105\n",
      "eval: split test . loss 2.733141e-02. error 3.94%. misses: 79\n",
      "epoch 16\n",
      "eval: split train. loss 1.065093e-02. error 1.47%. misses: 107\n",
      "eval: split test . loss 2.849034e-02. error 4.09%. misses: 82\n",
      "epoch 17\n",
      "eval: split train. loss 9.458693e-03. error 1.26%. misses: 92\n",
      "eval: split test . loss 2.668566e-02. error 3.79%. misses: 76\n",
      "epoch 18\n",
      "eval: split train. loss 7.680640e-03. error 1.08%. misses: 79\n",
      "eval: split test . loss 2.510950e-02. error 3.74%. misses: 75\n",
      "epoch 19\n",
      "eval: split train. loss 6.790097e-03. error 1.00%. misses: 73\n",
      "eval: split test . loss 2.578570e-02. error 3.69%. misses: 74\n",
      "epoch 20\n",
      "eval: split train. loss 6.345607e-03. error 0.93%. misses: 68\n",
      "eval: split test . loss 2.508449e-02. error 3.54%. misses: 71\n",
      "epoch 21\n",
      "eval: split train. loss 5.988171e-03. error 0.92%. misses: 66\n",
      "eval: split test . loss 2.509341e-02. error 3.59%. misses: 72\n",
      "epoch 22\n",
      "eval: split train. loss 5.771732e-03. error 0.88%. misses: 64\n",
      "eval: split test . loss 2.479325e-02. error 3.54%. misses: 71\n",
      "epoch 23\n",
      "eval: split train. loss 5.265484e-03. error 0.82%. misses: 60\n",
      "eval: split test . loss 2.467080e-02. error 3.69%. misses: 74\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
    "\n",
    "train(key, data, 23, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOaujFXQyqjd"
   },
   "source": [
    "Results:\n",
    "\n",
    "```\n",
    "epoch 23\n",
    "eval: split train. loss 5.265484e-03. error 0.82%. misses: 60\n",
    "eval: split test . loss 2.467080e-02. error 3.69%. misses: 74\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNEUr0vSfh1D"
   },
   "source": [
    "# \"Modern\" Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLDKA_qjhy6S"
   },
   "outputs": [],
   "source": [
    "!pip install -q flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m68J20OSh0rh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    from flax import linen as nn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax import linen as nn\n",
    "try:\n",
    "    from torchvision import datasets\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq torchvision\n",
    "    from torchvision import datasets\n",
    "\n",
    "\n",
    "def get_datasets(n_tr, n_te):\n",
    "    train_test = {}\n",
    "    for split in {\"train\", \"test\"}:\n",
    "        data = datasets.MNIST(\"./data\", train=split == \"train\", download=True)\n",
    "        n = n_tr if split == \"train\" else n_te\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        rp = jax.random.permutation(key, len(data))[:n]\n",
    "        X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
    "        Y = jnp.full((n, 10), 0, dtype=jnp.float32)\n",
    "        for i, ix in enumerate(rp):\n",
    "            I, yint = data[int(ix)]\n",
    "            xi = jnp.array(I, dtype=jnp.float32) / 127.5 - 1.0\n",
    "            xi = jax.image.resize(xi, (16, 16), \"bilinear\")\n",
    "            X = X.at[i].set(jnp.expand_dims(xi, axis=2))\n",
    "            Y = Y.at[i, yint].set(1.0)\n",
    "        train_test[split] = (X, Y)\n",
    "    return train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voQJfHf6thIH"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from flax import linen as nn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.linen.activation import tanh\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq optax\n",
    "    import optax\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    training: bool\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "    # sqrt(6) = 2.449... used by he_uniform() approximates Karpathy's 2.4\n",
    "    kernel_init: Callable = nn.initializers.he_uniform()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            augment_rng = self.make_rng(\"aug\")\n",
    "            shift_x, shift_y = jax.random.randint(augment_rng, (2,), -1, 2)\n",
    "            x = jnp.roll(x, (shift_x, shift_y), (1, 2))\n",
    "        x = jnp.pad(x, [(0, 0), (2, 2), (2, 2), (0, 0)], constant_values=-1.0)\n",
    "        x = nn.Conv(\n",
    "            features=12, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        bias1 = self.param(\"bias1\", self.bias_init, (8, 8, 12))\n",
    "        x = nn.relu(x + bias1)\n",
    "        x = jnp.pad(x, [(0, 0), (2, 2), (2, 2), (0, 0)], constant_values=-1.0)\n",
    "        x1, x2, x3 = (x[..., 0:8], x[..., 4:12], jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
    "        slice1 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x1)\n",
    "        slice2 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x2)\n",
    "        slice3 = nn.Conv(\n",
    "            features=4, kernel_size=(5, 5), strides=2, padding=\"VALID\", use_bias=False, kernel_init=self.kernel_init\n",
    "        )(x3)\n",
    "        x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
    "        bias2 = self.param(\"bias2\", self.bias_init, (4, 4, 12))\n",
    "        x = nn.relu(x + bias2)\n",
    "        x = nn.Dropout(0.25, deterministic=not self.training)(x)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=30, use_bias=False)(x)\n",
    "        bias3 = self.param(\"bias3\", self.bias_init, (30,))\n",
    "        x = nn.relu(x + bias3)\n",
    "        x = nn.Dense(features=10, use_bias=False)(x)\n",
    "        bias4 = self.param(\"bias4\", self.bias_init, (10,))\n",
    "        x = x + bias4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emEVm2HViete",
    "outputId": "b6a9bdd1-a9ea-4ae1-e779-9d2719e6514c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq optax\n",
    "    import optax\n",
    "try:\n",
    "    from flax.training import train_state\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax.training import train_state\n",
    "\n",
    "\n",
    "def learning_rate_fn(initial_rate, epochs, steps_per_epoch):\n",
    "    return optax.linear_schedule(\n",
    "        init_value=initial_rate, end_value=initial_rate / 3, transition_steps=epochs * steps_per_epoch\n",
    "    )\n",
    "\n",
    "\n",
    "def create_train_state(key, X, lr_fn):\n",
    "    model = Net(training=True)\n",
    "    key1, key2, key3 = jax.random.split(key, 3)\n",
    "    params = model.init({\"params\": key1, \"aug\": key2, \"dropout\": key3}, X)[\"params\"]\n",
    "    opt = optax.adamw(lr_fn)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=opt)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, X, Y, rng=jax.random.PRNGKey(0)):\n",
    "    aug_rng, dropout_rng = jax.random.split(jax.random.fold_in(rng, state.step))\n",
    "\n",
    "    def loss_fn(params):\n",
    "        Yhat = Net(training=True).apply({\"params\": params}, X, rngs={\"aug\": aug_rng, \"dropout\": dropout_rng})\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
    "        err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
    "        return loss, err\n",
    "\n",
    "    (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "\n",
    "def train_one_epoch(state, X, Y):\n",
    "    for step_num in range(X.shape[0]):\n",
    "        x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
    "        state = train_step(state, x, y)\n",
    "    return state\n",
    "\n",
    "\n",
    "def train(key, data, epochs, lr):\n",
    "    Xtr, Ytr = data[\"train\"]\n",
    "    Xte, Yte = data[\"test\"]\n",
    "    lr_fn = learning_rate_fn(lr, epochs, Xtr.shape[0])\n",
    "    train_state = create_train_state(key, Xtr, lr_fn)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch+1} with learning rate {lr_fn(train_state.step):.6f}\")\n",
    "        train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            eval_split(data, split, train_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRywm7o5jxPy"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, X, Y):\n",
    "    Yhat = Net(training=False).apply({\"params\": params}, X)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
    "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
    "    return loss, err\n",
    "\n",
    "\n",
    "def eval_split(data, split, params):\n",
    "    X, Y = data[split]\n",
    "    loss, err = eval_step(params, X, Y)\n",
    "    print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5D2wrzjEayd"
   },
   "outputs": [],
   "source": [
    "data = get_datasets(7291, 2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vncyC3wohuc",
    "outputId": "3a2c186a-ab9c-45bd-e27d-8817b86ee38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 with learning rate 0.000300\n",
      "eval: split train. loss 4.722151e-01. error 12.73%. misses: 928\n",
      "eval: split test . loss 4.376389e-01. error 11.81%. misses: 237\n",
      "epoch 2 with learning rate 0.000297\n",
      "eval: split train. loss 3.456218e-01. error 9.77%. misses: 712\n",
      "eval: split test . loss 3.105372e-01. error 8.87%. misses: 178\n",
      "epoch 3 with learning rate 0.000295\n",
      "eval: split train. loss 2.216365e-01. error 6.45%. misses: 469\n",
      "eval: split test . loss 1.981873e-01. error 5.53%. misses: 111\n",
      "epoch 4 with learning rate 0.000292\n",
      "eval: split train. loss 2.072843e-01. error 5.99%. misses: 437\n",
      "eval: split test . loss 1.910520e-01. error 5.48%. misses: 110\n",
      "epoch 5 with learning rate 0.000290\n",
      "eval: split train. loss 1.750381e-01. error 5.49%. misses: 399\n",
      "eval: split test . loss 1.611853e-01. error 4.93%. misses: 99\n",
      "epoch 6 with learning rate 0.000288\n",
      "eval: split train. loss 1.538368e-01. error 4.42%. misses: 321\n",
      "eval: split test . loss 1.411121e-01. error 4.19%. misses: 84\n",
      "epoch 7 with learning rate 0.000285\n",
      "eval: split train. loss 1.451264e-01. error 4.62%. misses: 337\n",
      "eval: split test . loss 1.325464e-01. error 4.09%. misses: 82\n",
      "epoch 8 with learning rate 0.000282\n",
      "eval: split train. loss 1.257392e-01. error 3.52%. misses: 257\n",
      "eval: split test . loss 1.164299e-01. error 3.34%. misses: 67\n",
      "epoch 9 with learning rate 0.000280\n",
      "eval: split train. loss 1.177755e-01. error 3.40%. misses: 248\n",
      "eval: split test . loss 1.107324e-01. error 3.69%. misses: 74\n",
      "epoch 10 with learning rate 0.000277\n",
      "eval: split train. loss 1.129500e-01. error 3.26%. misses: 237\n",
      "eval: split test . loss 1.068543e-01. error 3.14%. misses: 63\n",
      "epoch 11 with learning rate 0.000275\n",
      "eval: split train. loss 1.157665e-01. error 3.36%. misses: 245\n",
      "eval: split test . loss 1.119875e-01. error 3.34%. misses: 67\n",
      "epoch 12 with learning rate 0.000273\n",
      "eval: split train. loss 1.185108e-01. error 3.61%. misses: 263\n",
      "eval: split test . loss 1.146749e-01. error 3.69%. misses: 74\n",
      "epoch 13 with learning rate 0.000270\n",
      "eval: split train. loss 9.700271e-02. error 2.94%. misses: 214\n",
      "eval: split test . loss 9.375140e-02. error 3.04%. misses: 61\n",
      "epoch 14 with learning rate 0.000267\n",
      "eval: split train. loss 1.081733e-01. error 3.10%. misses: 226\n",
      "eval: split test . loss 1.054694e-01. error 3.24%. misses: 65\n",
      "epoch 15 with learning rate 0.000265\n",
      "eval: split train. loss 9.071133e-02. error 2.76%. misses: 201\n",
      "eval: split test . loss 8.586112e-02. error 2.64%. misses: 53\n",
      "epoch 16 with learning rate 0.000262\n",
      "eval: split train. loss 9.541860e-02. error 2.80%. misses: 203\n",
      "eval: split test . loss 9.335707e-02. error 3.19%. misses: 64\n",
      "epoch 17 with learning rate 0.000260\n",
      "eval: split train. loss 8.359449e-02. error 2.80%. misses: 203\n",
      "eval: split test . loss 8.113335e-02. error 2.79%. misses: 56\n",
      "epoch 18 with learning rate 0.000258\n",
      "eval: split train. loss 8.313517e-02. error 2.46%. misses: 179\n",
      "eval: split test . loss 8.725357e-02. error 2.64%. misses: 53\n",
      "epoch 19 with learning rate 0.000255\n",
      "eval: split train. loss 8.930960e-02. error 2.78%. misses: 203\n",
      "eval: split test . loss 8.548871e-02. error 2.79%. misses: 56\n",
      "epoch 20 with learning rate 0.000253\n",
      "eval: split train. loss 7.986999e-02. error 2.48%. misses: 181\n",
      "eval: split test . loss 7.389561e-02. error 2.64%. misses: 53\n",
      "epoch 21 with learning rate 0.000250\n",
      "eval: split train. loss 7.751217e-02. error 2.30%. misses: 168\n",
      "eval: split test . loss 7.085717e-02. error 2.44%. misses: 49\n",
      "epoch 22 with learning rate 0.000247\n",
      "eval: split train. loss 6.842067e-02. error 2.15%. misses: 157\n",
      "eval: split test . loss 6.652185e-02. error 2.29%. misses: 46\n",
      "epoch 23 with learning rate 0.000245\n",
      "eval: split train. loss 7.121788e-02. error 2.17%. misses: 158\n",
      "eval: split test . loss 6.131270e-02. error 1.79%. misses: 36\n",
      "epoch 24 with learning rate 0.000242\n",
      "eval: split train. loss 7.509596e-02. error 2.46%. misses: 179\n",
      "eval: split test . loss 6.493099e-02. error 2.14%. misses: 43\n",
      "epoch 25 with learning rate 0.000240\n",
      "eval: split train. loss 7.613951e-02. error 2.61%. misses: 190\n",
      "eval: split test . loss 7.143638e-02. error 2.19%. misses: 44\n",
      "epoch 26 with learning rate 0.000238\n",
      "eval: split train. loss 7.980061e-02. error 2.65%. misses: 193\n",
      "eval: split test . loss 7.566121e-02. error 2.34%. misses: 47\n",
      "epoch 27 with learning rate 0.000235\n",
      "eval: split train. loss 6.504884e-02. error 2.13%. misses: 155\n",
      "eval: split test . loss 5.958221e-02. error 1.99%. misses: 40\n",
      "epoch 28 with learning rate 0.000232\n",
      "eval: split train. loss 6.683959e-02. error 2.24%. misses: 163\n",
      "eval: split test . loss 6.922408e-02. error 2.59%. misses: 52\n",
      "epoch 29 with learning rate 0.000230\n",
      "eval: split train. loss 6.794566e-02. error 2.17%. misses: 158\n",
      "eval: split test . loss 6.709250e-02. error 2.44%. misses: 49\n",
      "epoch 30 with learning rate 0.000227\n",
      "eval: split train. loss 6.295200e-02. error 1.96%. misses: 143\n",
      "eval: split test . loss 5.890007e-02. error 2.54%. misses: 51\n",
      "epoch 31 with learning rate 0.000225\n",
      "eval: split train. loss 6.818665e-02. error 2.25%. misses: 164\n",
      "eval: split test . loss 6.444851e-02. error 2.24%. misses: 45\n",
      "epoch 32 with learning rate 0.000223\n",
      "eval: split train. loss 6.571253e-02. error 2.18%. misses: 159\n",
      "eval: split test . loss 6.434719e-02. error 2.44%. misses: 49\n",
      "epoch 33 with learning rate 0.000220\n",
      "eval: split train. loss 6.399426e-02. error 2.18%. misses: 159\n",
      "eval: split test . loss 6.240412e-02. error 2.24%. misses: 45\n",
      "epoch 34 with learning rate 0.000217\n",
      "eval: split train. loss 5.683114e-02. error 1.80%. misses: 131\n",
      "eval: split test . loss 5.610501e-02. error 1.84%. misses: 37\n",
      "epoch 35 with learning rate 0.000215\n",
      "eval: split train. loss 5.706797e-02. error 1.77%. misses: 129\n",
      "eval: split test . loss 6.036913e-02. error 2.34%. misses: 47\n",
      "epoch 36 with learning rate 0.000212\n",
      "eval: split train. loss 5.528478e-02. error 1.95%. misses: 142\n",
      "eval: split test . loss 5.302548e-02. error 2.04%. misses: 41\n",
      "epoch 37 with learning rate 0.000210\n",
      "eval: split train. loss 5.490229e-02. error 1.84%. misses: 133\n",
      "eval: split test . loss 5.376581e-02. error 1.94%. misses: 39\n",
      "epoch 38 with learning rate 0.000208\n",
      "eval: split train. loss 5.350880e-02. error 1.67%. misses: 122\n",
      "eval: split test . loss 5.158291e-02. error 1.79%. misses: 36\n",
      "epoch 39 with learning rate 0.000205\n",
      "eval: split train. loss 5.476158e-02. error 1.77%. misses: 129\n",
      "eval: split test . loss 5.336771e-02. error 1.69%. misses: 34\n",
      "epoch 40 with learning rate 0.000202\n",
      "eval: split train. loss 5.242018e-02. error 1.67%. misses: 122\n",
      "eval: split test . loss 5.161439e-02. error 1.89%. misses: 38\n",
      "epoch 41 with learning rate 0.000200\n",
      "eval: split train. loss 5.457530e-02. error 1.74%. misses: 126\n",
      "eval: split test . loss 6.135549e-02. error 2.44%. misses: 49\n",
      "epoch 42 with learning rate 0.000197\n",
      "eval: split train. loss 5.634554e-02. error 1.91%. misses: 139\n",
      "eval: split test . loss 6.446160e-02. error 2.34%. misses: 47\n",
      "epoch 43 with learning rate 0.000195\n",
      "eval: split train. loss 5.192847e-02. error 1.81%. misses: 132\n",
      "eval: split test . loss 6.171136e-02. error 2.14%. misses: 43\n",
      "epoch 44 with learning rate 0.000192\n",
      "eval: split train. loss 5.048798e-02. error 1.66%. misses: 121\n",
      "eval: split test . loss 5.762529e-02. error 1.94%. misses: 39\n",
      "epoch 45 with learning rate 0.000190\n",
      "eval: split train. loss 5.038778e-02. error 1.58%. misses: 114\n",
      "eval: split test . loss 5.986194e-02. error 2.09%. misses: 42\n",
      "epoch 46 with learning rate 0.000188\n",
      "eval: split train. loss 4.796446e-02. error 1.69%. misses: 122\n",
      "eval: split test . loss 5.005924e-02. error 1.84%. misses: 37\n",
      "epoch 47 with learning rate 0.000185\n",
      "eval: split train. loss 4.932489e-02. error 1.71%. misses: 125\n",
      "eval: split test . loss 5.289536e-02. error 2.24%. misses: 45\n",
      "epoch 48 with learning rate 0.000182\n",
      "eval: split train. loss 5.115648e-02. error 1.78%. misses: 129\n",
      "eval: split test . loss 5.819925e-02. error 2.04%. misses: 41\n",
      "epoch 49 with learning rate 0.000180\n",
      "eval: split train. loss 5.329847e-02. error 1.80%. misses: 131\n",
      "eval: split test . loss 5.682039e-02. error 2.09%. misses: 42\n",
      "epoch 50 with learning rate 0.000177\n",
      "eval: split train. loss 4.632418e-02. error 1.59%. misses: 116\n",
      "eval: split test . loss 5.570131e-02. error 2.09%. misses: 42\n",
      "epoch 51 with learning rate 0.000175\n",
      "eval: split train. loss 5.221667e-02. error 1.73%. misses: 126\n",
      "eval: split test . loss 6.282473e-02. error 2.14%. misses: 43\n",
      "epoch 52 with learning rate 0.000173\n",
      "eval: split train. loss 4.739231e-02. error 1.73%. misses: 126\n",
      "eval: split test . loss 5.634123e-02. error 1.99%. misses: 40\n",
      "epoch 53 with learning rate 0.000170\n",
      "eval: split train. loss 5.621015e-02. error 2.07%. misses: 151\n",
      "eval: split test . loss 6.867130e-02. error 2.19%. misses: 44\n",
      "epoch 54 with learning rate 0.000167\n",
      "eval: split train. loss 4.532041e-02. error 1.60%. misses: 117\n",
      "eval: split test . loss 5.811055e-02. error 1.99%. misses: 40\n",
      "epoch 55 with learning rate 0.000165\n",
      "eval: split train. loss 4.347728e-02. error 1.55%. misses: 113\n",
      "eval: split test . loss 5.601728e-02. error 2.14%. misses: 43\n",
      "epoch 56 with learning rate 0.000162\n",
      "eval: split train. loss 4.743553e-02. error 1.60%. misses: 117\n",
      "eval: split test . loss 6.145428e-02. error 2.24%. misses: 45\n",
      "epoch 57 with learning rate 0.000160\n",
      "eval: split train. loss 4.246239e-02. error 1.56%. misses: 114\n",
      "eval: split test . loss 5.335664e-02. error 1.79%. misses: 36\n",
      "epoch 58 with learning rate 0.000158\n",
      "eval: split train. loss 4.323665e-02. error 1.45%. misses: 106\n",
      "eval: split test . loss 5.636141e-02. error 1.89%. misses: 38\n",
      "epoch 59 with learning rate 0.000155\n",
      "eval: split train. loss 4.607718e-02. error 1.69%. misses: 122\n",
      "eval: split test . loss 5.969046e-02. error 2.14%. misses: 43\n",
      "epoch 60 with learning rate 0.000152\n",
      "eval: split train. loss 4.451877e-02. error 1.48%. misses: 108\n",
      "eval: split test . loss 5.823955e-02. error 1.99%. misses: 40\n",
      "epoch 61 with learning rate 0.000150\n",
      "eval: split train. loss 4.184551e-02. error 1.40%. misses: 101\n",
      "eval: split test . loss 5.383835e-02. error 1.69%. misses: 34\n",
      "epoch 62 with learning rate 0.000148\n",
      "eval: split train. loss 4.327311e-02. error 1.49%. misses: 109\n",
      "eval: split test . loss 5.188924e-02. error 1.84%. misses: 37\n",
      "epoch 63 with learning rate 0.000145\n",
      "eval: split train. loss 3.812368e-02. error 1.34%. misses: 97\n",
      "eval: split test . loss 4.565141e-02. error 1.69%. misses: 34\n",
      "epoch 64 with learning rate 0.000142\n",
      "eval: split train. loss 4.123368e-02. error 1.43%. misses: 103\n",
      "eval: split test . loss 5.299970e-02. error 1.84%. misses: 37\n",
      "epoch 65 with learning rate 0.000140\n",
      "eval: split train. loss 4.013669e-02. error 1.32%. misses: 95\n",
      "eval: split test . loss 5.678133e-02. error 1.99%. misses: 40\n",
      "epoch 66 with learning rate 0.000137\n",
      "eval: split train. loss 3.984843e-02. error 1.34%. misses: 97\n",
      "eval: split test . loss 5.329720e-02. error 2.09%. misses: 42\n",
      "epoch 67 with learning rate 0.000135\n",
      "eval: split train. loss 4.191425e-02. error 1.39%. misses: 101\n",
      "eval: split test . loss 5.370571e-02. error 1.99%. misses: 40\n",
      "epoch 68 with learning rate 0.000133\n",
      "eval: split train. loss 4.354529e-02. error 1.45%. misses: 106\n",
      "eval: split test . loss 5.472580e-02. error 1.99%. misses: 40\n",
      "epoch 69 with learning rate 0.000130\n",
      "eval: split train. loss 3.600218e-02. error 1.25%. misses: 91\n",
      "eval: split test . loss 5.039397e-02. error 2.14%. misses: 43\n",
      "epoch 70 with learning rate 0.000127\n",
      "eval: split train. loss 3.712326e-02. error 1.14%. misses: 83\n",
      "eval: split test . loss 4.781391e-02. error 1.79%. misses: 36\n",
      "epoch 71 with learning rate 0.000125\n",
      "eval: split train. loss 4.377073e-02. error 1.43%. misses: 103\n",
      "eval: split test . loss 5.955317e-02. error 2.04%. misses: 41\n",
      "epoch 72 with learning rate 0.000123\n",
      "eval: split train. loss 4.096783e-02. error 1.41%. misses: 103\n",
      "eval: split test . loss 5.084800e-02. error 1.94%. misses: 39\n",
      "epoch 73 with learning rate 0.000120\n",
      "eval: split train. loss 3.989225e-02. error 1.32%. misses: 95\n",
      "eval: split test . loss 5.022623e-02. error 2.09%. misses: 42\n",
      "epoch 74 with learning rate 0.000117\n",
      "eval: split train. loss 3.819638e-02. error 1.43%. misses: 103\n",
      "eval: split test . loss 4.982632e-02. error 1.99%. misses: 40\n",
      "epoch 75 with learning rate 0.000115\n",
      "eval: split train. loss 3.834034e-02. error 1.29%. misses: 94\n",
      "eval: split test . loss 4.789943e-02. error 1.64%. misses: 33\n",
      "epoch 76 with learning rate 0.000112\n",
      "eval: split train. loss 3.586408e-02. error 1.21%. misses: 88\n",
      "eval: split test . loss 4.683260e-02. error 1.69%. misses: 34\n",
      "epoch 77 with learning rate 0.000110\n",
      "eval: split train. loss 3.496870e-02. error 1.12%. misses: 82\n",
      "eval: split test . loss 4.608429e-02. error 1.54%. misses: 31\n",
      "epoch 78 with learning rate 0.000108\n",
      "eval: split train. loss 3.359542e-02. error 1.07%. misses: 78\n",
      "eval: split test . loss 4.598244e-02. error 1.69%. misses: 34\n",
      "epoch 79 with learning rate 0.000105\n",
      "eval: split train. loss 3.431604e-02. error 1.15%. misses: 84\n",
      "eval: split test . loss 4.517807e-02. error 1.59%. misses: 32\n",
      "epoch 80 with learning rate 0.000102\n",
      "eval: split train. loss 3.316079e-02. error 1.06%. misses: 77\n",
      "eval: split test . loss 4.969697e-02. error 1.74%. misses: 35\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
    "\n",
    "train(key, data, 80, 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ey5_RPUytFV"
   },
   "source": [
    "Change 1: replace tanh on last layer with FC and use softmax. Lower learning rate to 0.01\n",
    "\n",
    "```\n",
    "epoch 23\n",
    "eval: split train. loss 7.162272e-03. error 0.05%. misses: 4\n",
    "eval: split test . loss 1.687743e-01. error 4.14%. misses: 83\n",
    "\n",
    "```\n",
    "\n",
    "Change 2: change from SGD to AdamW with LR 3e-4, double epochs to 46, decay LR to 1e-4 over the course of training.\n",
    "\n",
    "```\n",
    "epoch 46\n",
    "eval: split train. loss 1.890260e-03. error 0.04%. misses: 2\n",
    "eval: split test . loss 1.953933e-01. error 4.04%. misses: 81\n",
    "```\n",
    "\n",
    "Change 3: Introduce data augmentation, e.g. a shift by at most 1 pixel in both x/y directions, and bump up training time to 60 epochs.\n",
    "\n",
    "```\n",
    "epoch 60\n",
    "eval: split train. loss 5.098452e-02. error 1.65%. misses: 120\n",
    "eval: split test . loss 9.166716e-02. error 2.59%. misses: 52\n",
    "```\n",
    "\n",
    "Change 4: add dropout at layer H3, shift activation function to relu, and bring up iterations to 80.\n",
    "\n",
    "```\n",
    "epoch 80\n",
    "eval: split train. loss 3.316079e-02. error 1.06%. misses: 77\n",
    "eval: split test . loss 4.969697e-02. error 1.74%. misses: 35\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lecun_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
