{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/flow_spline_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gJIYeg2Y7oZv"
            },
            "source": [
                "# Spline Flow using JAX, Haiku, Optax and Distrax\n",
                "\n",
                "In this notebook we will implement Spline flow to fit a distribution to MNIST dataset. We will be using the RationalQuadraticSpline, a piecewise rational quadratic spline, and Masked Couplings as explained in paper [Neural Spline Flows](https://arxiv.org/abs/1906.04032) by Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. \n",
                "\n",
                "This notebook replicates the [original distrax code ](https://github.com/deepmind/distrax/blob/master/examples/flow.py) with suitable minor modifications.\n",
                " \n",
                "\n",
                "\n",
                "For implementing the Quadratic Splines with Coupling flows, We will be using following libraries:\n",
                "- JAX  - NumPy on GPU, and TPU with automatic differentiation.\n",
                "- Haiku - JAX based Neural Network Library.\n",
                "- Optax -  gradient processing and optimization library for JAX.\n",
                "- Distrax - a lightweight library of probability distributions and bijectors.\n",
                "\n",
                "### Installing required libraries in Colab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "l0K_D4QX74gV",
                "outputId": "bcdb6f99-11d9-4aa8-d1c6-fad24b9caf60"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
                        "Requirement already satisfied: distrax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
                        "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.7/dist-packages (0.0.6)\n",
                        "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (3.10.0.2)\n",
                        "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.0.0)\n",
                        "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.0+cuda11.cudnn805)\n",
                        "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.1)\n",
                        "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
                        "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.5)\n",
                        "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->optax) (1.15.0)\n",
                        "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
                        "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
                        "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
                        "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.4.1)\n",
                        "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
                        "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from distrax) (0.16.0)\n",
                        "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.15.0->distrax) (1.3.0)\n",
                        "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
                        "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.5.3)\n",
                        "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.0.2)\n",
                        "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.9)\n"
                    ]
                }
            ],
            "source": [
                "!pip install -qq -U optax distrax dm-haiku"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "i8vGb5NE-sWj"
            },
            "source": [
                "### Importing all required libraries and packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "T6QrYSdV7oZ3"
            },
            "outputs": [],
            "source": [
                "from typing import Any, Iterator, Mapping, Optional, Sequence, Tuple\n",
                "\n",
                "try:\n",
                "    import distrax\n",
                "except ModuleNotFoundError:\n",
                "    %pip install -qq distrax\n",
                "    import distrax\n",
                "try:\n",
                "    import haiku as hk\n",
                "except ModuleNotFoundError:\n",
                "    %pip install -qq dm-haiku\n",
                "    import haiku as hk\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import numpy as np\n",
                "\n",
                "try:\n",
                "    import optax\n",
                "except ModuleNotFoundError:\n",
                "    %pip install -qq optax\n",
                "    import optax\n",
                "try:\n",
                "    import tensorflow_datasets as tfds\n",
                "except ModuleNotFoundError:\n",
                "    %pip install -qq tensorflow tensorflow_datasets\n",
                "    import tensorflow_datasets as tfds\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "Array = jnp.ndarray\n",
                "PRNGKey = Array\n",
                "Batch = Mapping[str, np.ndarray]\n",
                "OptState = Any\n",
                "\n",
                "MNIST_IMAGE_SHAPE = (28, 28, 1)\n",
                "batch_size = 128"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7sF08dbZ7oZ7"
            },
            "source": [
                "### Conditioner\n",
                "\n",
                "Let $u \\in \\mathbb{R}^D $ be the input. The input is split into two equal sub spaces $(x^A, x^B)$  each of size $\\mathbb{R}^{d}$ such that $d = D/2$.\n",
                "\n",
                "Let us assume we have a bijection $\\hat{f}(\\cdot;\\theta): \\mathbb{R}^d \\to \\mathbb{R}^d$ parameterized by $\\theta $\n",
                "\n",
                "We define a single coupling layer as a function $ f: \\mathbb{R}^D \\to \\mathbb{R}^D $ given by $x = f(u)$ as below:\n",
                "\n",
                "$ x^A = \\hat{f}(u^A; \\Theta(u^B))$\n",
                "\n",
                "$ x^B = u^B $\n",
                "\n",
                "$ x = (x^A, x^B)$\n",
                "\n",
                "In other words, the input $u$ is split into $(u^A, u^B)$ and output $(x^A, x^B)$ is combined back into $x$ using a binary mask $m$. Therefore, the single coupling layer $ f: \\mathbb{R}^D \\to \\mathbb{R}^D $ given by $x = f(u)$ is defined in a single equation as below:\n",
                "\n",
                "$x = f(u) = b \\odot u + (1-b) \\hat{f}(u; \\Theta(b \\odot u))$\n",
                "\n",
                "We will implement the full flow by chaining multiple coupling layers. The mask $b$ will be flipped between each layer to ensure we capture dependencies in more expressive way. \n",
                "\n",
                "The function **${\\Theta}$** is called the **Conditioner** which we implement with a set of Linear layers and ReLU activation functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "kE3YTziR7oZ8"
            },
            "outputs": [],
            "source": [
                "def make_conditioner(\n",
                "    event_shape: Sequence[int], hidden_sizes: Sequence[int], num_bijector_params: int\n",
                ") -> hk.Sequential:\n",
                "    \"\"\"Creates an MLP conditioner for each layer of the flow.\"\"\"\n",
                "    return hk.Sequential(\n",
                "        [\n",
                "            hk.Flatten(preserve_dims=-len(event_shape)),\n",
                "            hk.nets.MLP(hidden_sizes, activate_final=True),\n",
                "            # We initialize this linear layer to zero so that the flow is initialized\n",
                "            # to the identity function.\n",
                "            hk.Linear(np.prod(event_shape) * num_bijector_params, w_init=jnp.zeros, b_init=jnp.zeros),\n",
                "            hk.Reshape(tuple(event_shape) + (num_bijector_params,), preserve_dims=-1),\n",
                "        ]\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_b4hkTxK7oZ-"
            },
            "source": [
                "### Flow Model\n",
                "\n",
                "Next we implement the **Bijector** $\\hat{f}$ using `distrax.RationalQuadraticSpline` and the **Masked Coupling** $f$ using `distrax.MaskedCoupling`\n",
                "\n",
                "We join together sequentailly a number of masked coupling layers to define the complete Spline FLow. \n",
                "\n",
                "We define base distribution of our flow as **Uniform distribution**. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FGkKBR8n7oZ-"
            },
            "outputs": [],
            "source": [
                "def make_flow_model(\n",
                "    event_shape: Sequence[int], num_layers: int, hidden_sizes: Sequence[int], num_bins: int\n",
                ") -> distrax.Transformed:\n",
                "    \"\"\"Creates the flow model.\"\"\"\n",
                "    # Alternating binary mask.\n",
                "    mask = jnp.arange(0, np.prod(event_shape)) % 2\n",
                "    mask = jnp.reshape(mask, event_shape)\n",
                "    mask = mask.astype(bool)\n",
                "\n",
                "    def bijector_fn(params: Array):\n",
                "        return distrax.RationalQuadraticSpline(params, range_min=0.0, range_max=1.0)\n",
                "\n",
                "    # Number of parameters for the rational-quadratic spline:\n",
                "    # - `num_bins` bin widths\n",
                "    # - `num_bins` bin heights\n",
                "    # - `num_bins + 1` knot slopes\n",
                "    # for a total of `3 * num_bins + 1` parameters.\n",
                "    num_bijector_params = 3 * num_bins + 1\n",
                "\n",
                "    layers = []\n",
                "    for _ in range(num_layers):\n",
                "        layer = distrax.MaskedCoupling(\n",
                "            mask=mask,\n",
                "            bijector=bijector_fn,\n",
                "            conditioner=make_conditioner(event_shape, hidden_sizes, num_bijector_params),\n",
                "        )\n",
                "        layers.append(layer)\n",
                "        # Flip the mask after each layer.\n",
                "        mask = jnp.logical_not(mask)\n",
                "\n",
                "    # We invert the flow so that the `forward` method is called with `log_prob`.\n",
                "    flow = distrax.Inverse(distrax.Chain(layers))\n",
                "    base_distribution = distrax.Independent(\n",
                "        distrax.Uniform(low=jnp.zeros(event_shape), high=jnp.ones(event_shape)),\n",
                "        reinterpreted_batch_ndims=len(event_shape),\n",
                "    )\n",
                "\n",
                "    return distrax.Transformed(base_distribution, flow)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "M4LcgwQ47oaA"
            },
            "source": [
                "### Data Loading and preparation\n",
                "In this cell, we define a function to load the MNIST dataset using TFDS (Tensorflow Datasets) package.\n",
                "\n",
                "We also have a function `prepare_data` to: \n",
                "1. dequantize the data i.e. to convert the integer pixel values from `{0,1,...,255}` to real number values `[0,256)` by adding a random uniform noise `[0,1)`; and \n",
                "\n",
                "2. Normalize the pixel values from `[0,256)` to `[0,1)`  \n",
                "\n",
                "The dequantization of data is done only at training time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "XGqQqhvt7oaA"
            },
            "outputs": [],
            "source": [
                "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
                "    ds = tfds.load(\"mnist\", split=split, shuffle_files=True)\n",
                "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
                "    ds = ds.batch(batch_size)\n",
                "    ds = ds.prefetch(buffer_size=5)\n",
                "    ds = ds.repeat()\n",
                "    return iter(tfds.as_numpy(ds))\n",
                "\n",
                "\n",
                "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
                "    data = batch[\"image\"].astype(np.float32)\n",
                "    if prng_key is not None:\n",
                "        # Dequantize pixel values {0, 1, ..., 255} with uniform noise [0, 1).\n",
                "        data += jax.random.uniform(prng_key, data.shape)\n",
                "    return data / 256.0  # Normalize pixel values from [0, 256) to [0, 1)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zC1jKZXH7oaB"
            },
            "source": [
                "### Log Probability, Sample and training loss Functions\n",
                "Next we define the `log_prob` `model_sample` and `loss_fn`. `log_prob` is responsible for calculating the log of the probability of the data which we want to maximize for MNIST data inside `loss_fn`.\n",
                "\n",
                "`model_sample` allows us to sample new data points after the model has been trained on MNIST. FOr a well trained model, these samples will look like MNIST digits generated synthetically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "izlGzJdH7oaD"
            },
            "outputs": [],
            "source": [
                "flow_num_layers = 8\n",
                "mlp_num_layers = 2\n",
                "hidden_size = 500\n",
                "num_bins = 4\n",
                "learning_rate = 1e-4\n",
                "\n",
                "# using 100,000 steps could take long (about 2 hours) but will give better results.\n",
                "# You can try with 10,000 steps to run it fast but result may not be very good\n",
                "\n",
                "training_steps = 10000\n",
                "eval_frequency = 1000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "JZ2DXvH87oaD"
            },
            "outputs": [],
            "source": [
                "@hk.without_apply_rng\n",
                "@hk.transform\n",
                "def log_prob(data: Array) -> Array:\n",
                "    model = make_flow_model(\n",
                "        event_shape=MNIST_IMAGE_SHAPE,\n",
                "        num_layers=flow_num_layers,\n",
                "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
                "        num_bins=num_bins,\n",
                "    )\n",
                "    return model.log_prob(data)\n",
                "\n",
                "\n",
                "@hk.without_apply_rng\n",
                "@hk.transform\n",
                "def model_sample(key: PRNGKey, num_samples: int) -> Array:\n",
                "    model = make_flow_model(\n",
                "        event_shape=MNIST_IMAGE_SHAPE,\n",
                "        num_layers=flow_num_layers,\n",
                "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
                "        num_bins=num_bins,\n",
                "    )\n",
                "    return model.sample(seed=key, sample_shape=[num_samples])\n",
                "\n",
                "\n",
                "def loss_fn(params: hk.Params, prng_key: PRNGKey, batch: Batch) -> Array:\n",
                "    data = prepare_data(batch, prng_key)\n",
                "    # Loss is average negative log likelihood.\n",
                "    loss = -jnp.mean(log_prob.apply(params, data))\n",
                "    return loss\n",
                "\n",
                "\n",
                "@jax.jit\n",
                "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
                "    data = prepare_data(batch)  # We don't dequantize during evaluation.\n",
                "    loss = -jnp.mean(log_prob.apply(params, data))\n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ldAqassc7oaE"
            },
            "source": [
                "### Training   \n",
                "Next we define, the `update` function for the gradient update. We use `jax.grad` to calculate the gradient of loss wrt model parameters.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "GisFglnr7oaF"
            },
            "outputs": [],
            "source": [
                "optimizer = optax.adam(learning_rate)\n",
                "\n",
                "\n",
                "@jax.jit\n",
                "def update(params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch) -> Tuple[hk.Params, OptState]:\n",
                "    \"\"\"Single SGD update step.\"\"\"\n",
                "    grads = jax.grad(loss_fn)(params, prng_key, batch)\n",
                "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
                "    new_params = optax.apply_updates(params, updates)\n",
                "    return new_params, new_opt_state"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IvWrGKTgNaSD"
            },
            "source": [
                "Now we carry out the training of the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "kBvaapIO7oaF",
                "outputId": "12cf22ea-4155-4c46-dc5e-94f2b3345736"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "STEP:     0; Validation loss: -5.243\n",
                        "STEP:  1000; Validation loss: -3330.548\n",
                        "STEP:  2000; Validation loss: -3316.721\n",
                        "STEP:  3000; Validation loss: -3319.474\n",
                        "STEP:  4000; Validation loss: -3321.137\n",
                        "STEP:  5000; Validation loss: -3297.604\n",
                        "STEP:  6000; Validation loss: -3295.713\n",
                        "STEP:  7000; Validation loss: -3260.267\n",
                        "STEP:  8000; Validation loss: -3263.344\n",
                        "STEP:  9000; Validation loss: -3286.821\n",
                        "STEP: 10000; Validation loss: -3301.308\n",
                        "STEP: 11000; Validation loss: -3296.085\n",
                        "STEP: 12000; Validation loss: -3310.035\n",
                        "STEP: 13000; Validation loss: -3326.781\n",
                        "STEP: 14000; Validation loss: -3305.100\n",
                        "STEP: 15000; Validation loss: -3317.956\n",
                        "STEP: 16000; Validation loss: -3365.609\n",
                        "STEP: 17000; Validation loss: -3339.233\n",
                        "STEP: 18000; Validation loss: -3346.478\n",
                        "STEP: 19000; Validation loss: -3325.882\n",
                        "STEP: 20000; Validation loss: -3340.056\n",
                        "STEP: 21000; Validation loss: -3342.137\n",
                        "STEP: 22000; Validation loss: -3338.241\n",
                        "STEP: 23000; Validation loss: -3354.057\n",
                        "STEP: 24000; Validation loss: -3384.099\n",
                        "STEP: 25000; Validation loss: -3326.196\n",
                        "STEP: 26000; Validation loss: -3367.737\n",
                        "STEP: 27000; Validation loss: -3353.810\n",
                        "STEP: 28000; Validation loss: -3403.825\n",
                        "STEP: 29000; Validation loss: -3367.897\n",
                        "STEP: 30000; Validation loss: -3384.129\n",
                        "STEP: 31000; Validation loss: -3395.217\n",
                        "STEP: 32000; Validation loss: -3426.372\n",
                        "STEP: 33000; Validation loss: -3381.938\n",
                        "STEP: 34000; Validation loss: -3397.225\n",
                        "STEP: 35000; Validation loss: -3406.573\n",
                        "STEP: 36000; Validation loss: -3395.962\n",
                        "STEP: 37000; Validation loss: -3371.118\n",
                        "STEP: 38000; Validation loss: -3405.979\n",
                        "STEP: 39000; Validation loss: -3394.809\n",
                        "STEP: 40000; Validation loss: -3373.334\n",
                        "STEP: 41000; Validation loss: -3388.248\n",
                        "STEP: 42000; Validation loss: -3407.083\n",
                        "STEP: 43000; Validation loss: -3404.585\n",
                        "STEP: 44000; Validation loss: -3400.513\n",
                        "STEP: 45000; Validation loss: -3403.710\n",
                        "STEP: 46000; Validation loss: -3420.211\n",
                        "STEP: 47000; Validation loss: -3408.354\n",
                        "STEP: 48000; Validation loss: -3442.830\n",
                        "STEP: 49000; Validation loss: -3444.429\n",
                        "STEP: 50000; Validation loss: -3388.476\n",
                        "STEP: 51000; Validation loss: -3432.037\n",
                        "STEP: 52000; Validation loss: -3424.902\n",
                        "STEP: 53000; Validation loss: -3415.527\n",
                        "STEP: 54000; Validation loss: -3410.981\n",
                        "STEP: 55000; Validation loss: -3437.098\n",
                        "STEP: 56000; Validation loss: -3404.357\n",
                        "STEP: 57000; Validation loss: -3397.224\n",
                        "STEP: 58000; Validation loss: -3414.235\n",
                        "STEP: 59000; Validation loss: -3448.812\n",
                        "STEP: 60000; Validation loss: -3392.534\n",
                        "STEP: 61000; Validation loss: -3390.733\n",
                        "STEP: 62000; Validation loss: -3419.801\n",
                        "STEP: 63000; Validation loss: -3416.396\n",
                        "STEP: 64000; Validation loss: -3416.169\n",
                        "STEP: 65000; Validation loss: -3384.039\n",
                        "STEP: 66000; Validation loss: -3419.544\n",
                        "STEP: 67000; Validation loss: -3408.180\n",
                        "STEP: 68000; Validation loss: -3410.440\n",
                        "STEP: 69000; Validation loss: -3414.480\n",
                        "STEP: 70000; Validation loss: -3399.575\n",
                        "STEP: 71000; Validation loss: -3412.096\n",
                        "STEP: 72000; Validation loss: -3440.463\n",
                        "STEP: 73000; Validation loss: -3420.625\n",
                        "STEP: 74000; Validation loss: -3425.068\n",
                        "STEP: 75000; Validation loss: -3431.229\n",
                        "STEP: 76000; Validation loss: -3415.992\n",
                        "STEP: 77000; Validation loss: -3404.405\n",
                        "STEP: 78000; Validation loss: -3469.461\n",
                        "STEP: 79000; Validation loss: -3391.149\n",
                        "STEP: 80000; Validation loss: -3392.615\n",
                        "STEP: 81000; Validation loss: -3423.543\n",
                        "STEP: 82000; Validation loss: -3413.071\n",
                        "STEP: 83000; Validation loss: -3441.366\n",
                        "STEP: 84000; Validation loss: -3414.967\n",
                        "STEP: 85000; Validation loss: -3396.721\n",
                        "STEP: 86000; Validation loss: -3409.096\n",
                        "STEP: 87000; Validation loss: -3431.324\n",
                        "STEP: 88000; Validation loss: -3437.945\n",
                        "STEP: 89000; Validation loss: -3433.234\n",
                        "STEP: 90000; Validation loss: -3422.854\n",
                        "STEP: 91000; Validation loss: -3404.757\n",
                        "STEP: 92000; Validation loss: -3427.891\n",
                        "STEP: 93000; Validation loss: -3431.066\n",
                        "STEP: 94000; Validation loss: -3439.144\n",
                        "STEP: 95000; Validation loss: -3432.875\n",
                        "STEP: 96000; Validation loss: -3410.505\n",
                        "STEP: 97000; Validation loss: -3375.825\n",
                        "STEP: 98000; Validation loss: -3402.304\n",
                        "STEP: 99000; Validation loss: -3391.469\n"
                    ]
                }
            ],
            "source": [
                "prng_seq = hk.PRNGSequence(42)\n",
                "params = log_prob.init(next(prng_seq), np.zeros((1, *MNIST_IMAGE_SHAPE)))\n",
                "opt_state = optimizer.init(params)\n",
                "train_ds = load_dataset(tfds.Split.TRAIN, batch_size)\n",
                "valid_ds = load_dataset(tfds.Split.TEST, batch_size)\n",
                "for step in range(training_steps):\n",
                "    params, opt_state = update(params, next(prng_seq), opt_state, next(train_ds))\n",
                "\n",
                "    if step % eval_frequency == 0:\n",
                "        val_loss = eval_fn(params, next(valid_ds))\n",
                "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dlA6fiYn7oaH"
            },
            "source": [
                "### Sampling from Trained Flow Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "aczbVzmAOlH4"
            },
            "source": [
                "### Plot new samples\n",
                "After the model has been trained in MNIST, we draw new samples and plot them. Once the model has been trained enough, these should look like MNIST dataset digits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 246
                },
                "id": "Qt59aIVl7oaI",
                "outputId": "62ef5147-c177-465c-82a8-471a784a4f6b"
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": [
                            "<Figure size 720x288 with 10 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def plot_batch(batch: Batch) -> None:\n",
                "    \"\"\"Plots a batch of MNIST digits.\"\"\"\n",
                "    images = batch.reshape((-1,) + MNIST_IMAGE_SHAPE)\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    for i in range(10):\n",
                "        plt.subplot(2, 5, i + 1)\n",
                "        plt.imshow(np.squeeze(images[i]), cmap=\"gray\")\n",
                "        plt.axis(\"off\")\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "sample = model_sample.apply(params, next(prng_seq), num_samples=10)\n",
                "plot_batch(sample)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "include_colab_link": true,
            "name": "mnist_spline_flow.ipynb",
            "provenance": []
        },
        "interpreter": {
            "hash": "b444a8a50c005290911b7a1b2bedac1d9c40e4ef30d4a73bdc576c10e0bd6118"
        },
        "kernelspec": {
            "display_name": "Python 3.7.11 ('probml')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}