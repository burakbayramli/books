{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358908ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file shows a demo implementation of kernel classifiers like L1,L2-logreg,\n",
    "# SVC, RVC.\n",
    "# Author Srikar Reddy Jilugu(@always-newbie161)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from sklearn.svm import SVC\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq scikit-learn\n",
    "    from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "from probml_utils.rvm_classifier import RVC  # Core implementation.\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "\n",
    "N = 200\n",
    "X, y = make_moons(n_samples=N, noise=0.3, random_state=10)\n",
    "# X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Feature Mapping X to rbf_features to simulate non-linear logreg using linear ones.\n",
    "rbf_feature = RBFSampler(gamma=0.3, random_state=1, n_components=N)\n",
    "X_rbf = rbf_feature.fit_transform(X)\n",
    "\n",
    "# Using CV to find SVM regularization parameter.\n",
    "C = np.power(2, np.linspace(-5, 5, 10))\n",
    "mean_scores = [cross_val_score(SVC(kernel=\"rbf\", gamma=0.3, C=c), X, y, cv=5).mean() for c in C]\n",
    "c = C[np.argmax(mean_scores)]\n",
    "print(\"SVM c= \", c)\n",
    "\n",
    "classifiers = {\n",
    "    \"logregL2\": LogisticRegression(C=c, penalty=\"l2\", solver=\"saga\", multi_class=\"ovr\", max_iter=10000),\n",
    "    \"logregL1\": LogisticRegression(C=c, penalty=\"l1\", solver=\"saga\", multi_class=\"ovr\", max_iter=10000),\n",
    "    \"RVM\": RVC(),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", gamma=0.3, C=c, probability=True),\n",
    "}\n",
    "\n",
    "h = 0.05  # step size in the mesh\n",
    "\n",
    "# Mesh to use in the boundary plotting.\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "def plot_scatters(X, y):\n",
    "    for class_value in range(2):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix = np.where(y == class_value)\n",
    "        # creating scatter of these samples\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap=\"Paired\", marker=\"X\", s=30)\n",
    "\n",
    "\n",
    "def plot_SVs(SV):\n",
    "    plt.scatter(SV[:, 0], SV[:, 1], s=100, facecolor=\"none\", edgecolor=\"green\")\n",
    "\n",
    "\n",
    "levels = [0.5]\n",
    "# levels = np.linspace(0, 1, 5)\n",
    "\n",
    "for (name, clf) in classifiers.items():\n",
    "\n",
    "    if name == \"logregL2\":\n",
    "        clf.fit(X_rbf, y)\n",
    "        Z = clf.predict_proba(rbf_feature.fit_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z[:, 0].reshape(xx.shape)\n",
    "        plt.title(name + \", nerr= {}\".format(np.sum(y != clf.predict(X_rbf))))\n",
    "        plt.contour(xx, yy, Z, levels)\n",
    "        plot_scatters(X, y)\n",
    "        pml.savefig(\"kernelBinaryClassifDemo{}.pdf\".format(name), dpi=300)\n",
    "        plt.show()\n",
    "    elif name == \"logregL1\":\n",
    "        clf.fit(X_rbf, y)\n",
    "        Z = clf.predict_proba(rbf_feature.fit_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z[:, 0].reshape(xx.shape)\n",
    "        plt.contour(xx, yy, Z, levels)\n",
    "        plot_scatters(X, y)\n",
    "        conf_scores = np.abs(clf.decision_function(X_rbf))\n",
    "        SV = X[(conf_scores > conf_scores.mean())]\n",
    "        nsupport = SV.shape[0]\n",
    "        nerr = np.sum(y != clf.predict(X_rbf))\n",
    "        plot_SVs(SV)\n",
    "        plt.title(f\"{name}, nerr={nerr}, nsupport={nsupport}\")\n",
    "        pml.savefig(\"kernelBinaryClassifDemo{}.pdf\".format(name), dpi=300)\n",
    "        plt.show()\n",
    "    elif name == \"RVM\":\n",
    "        clf.fit(X, y)\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, Z, levels)\n",
    "        plot_scatters(X, y)\n",
    "        SV = clf.relevance_vectors_\n",
    "        plot_SVs(SV)\n",
    "        nsupport = SV.shape[0]\n",
    "        nerr = np.sum(y != clf.predict(X))\n",
    "        plt.title(f\"{name}, nerr={nerr}, nsupport={nsupport}\")\n",
    "        pml.savefig(\"kernelBinaryClassifDemo{}.pdf\".format(name), dpi=300)\n",
    "        plt.show()\n",
    "    elif name == \"SVM\":\n",
    "        clf.fit(X, y)\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z[:, 0]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, Z, levels)\n",
    "        plot_scatters(X, y)\n",
    "        SV = clf.support_vectors_\n",
    "        plot_SVs(SV)\n",
    "        nsupport = SV.shape[0]\n",
    "        nerr = np.sum(y != clf.predict(X))\n",
    "        plt.title(f\"{name}, nerr={nerr}, nsupport={nsupport}\")\n",
    "        pml.savefig(\"kernelBinaryClassifDemo{}.pdf\".format(name), dpi=300)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
