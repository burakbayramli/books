{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4be815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# http://www.pitt.edu/~naraehan/presentation/Movie+Reviews+sentiment+analysis+with+Scikit-Learn.html\n",
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first example.\",\n",
    "    \"This example is the second example\",\n",
    "    \"Do you want to see more examples, or is three examples enough?\",\n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# default tokenizer drops important words, NLTK tokenzier keeps everything,\n",
    "try:\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq nltk\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = lambda s: RegexpTokenizer(r\"\\w+\").tokenize(s)  # alphanumeric strings get tokenized\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "B = vectorizer.fit_transform(corpus).todense()  # bag of words, (N,T)\n",
    "print(vectorizer.get_feature_names())\n",
    "[\n",
    "    \"do\",\n",
    "    \"enough\",\n",
    "    \"example\",\n",
    "    \"examples\",\n",
    "    \"first\",\n",
    "    \"is\",\n",
    "    \"more\",\n",
    "    \"or\",\n",
    "    \"second\",\n",
    "    \"see\",\n",
    "    \"the\",\n",
    "    \"this\",\n",
    "    \"three\",\n",
    "    \"to\",\n",
    "    \"want\",\n",
    "    \"you\",\n",
    "]\n",
    "\n",
    "print(B)\n",
    "\"\"\"\n",
    "[[0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0]\n",
    " [0 0 2 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
    " [1 1 0 2 0 1 1 1 0 1 0 0 1 1 1 1]]\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq tensorflow\n",
    "    from tensorflow import keras\n",
    "\n",
    "t = keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts(corpus)\n",
    "print(t.document_count)\n",
    "print(t.word_counts)\n",
    "print(t.word_docs)\n",
    "print(t.word_index)\n",
    "\"\"\"\n",
    "3\n",
    "OrderedDict([('this', 2), ('is', 3), ('the', 2), ('first', 1), ('example', 3),\n",
    " ('second', 1), ('do', 1), ('you', 1), ('want', 1), ('to', 1),\n",
    " ('see', 1), ('more', 1), ('examples', 2), ('or', 1), ('three', 1), ('enough', 1)])\n",
    "defaultdict(<class 'int'>, {'first': 1, 'the': 2, 'is': 3, 'this': 2, 'example': 2, \n",
    "'second': 1, 'you': 1, 'see': 1, 'do': 1, 'or': 1, 'examples': 1, 'enough': 1,\n",
    " 'three': 1, 'more': 1, 'want': 1, 'to': 1})\n",
    "{'is': 1, 'example': 2, 'this': 3, 'the': 4, 'examples': 5, 'first': 6, \n",
    "'second': 7, 'do': 8, 'you': 9, 'want': 10, 'to': 11, 'see': 12, 'more': 13,\n",
    " 'or': 14, 'three': 15, 'enough': 16}\n",
    "\"\"\"\n",
    "\n",
    "encoded_docs = t.texts_to_matrix(corpus, mode=\"count\")\n",
    "print(encoded_docs)\n",
    "\"\"\"\n",
    "[[0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 2. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 2. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
    "\"\"\"\n",
    "reverse_word_index = dict([(value, key) for (key, value) in t.word_index.items()])\n",
    "\"\"\"\n",
    "{1: 'is',\n",
    " 2: 'example',\n",
    " 3: 'this',\n",
    " 4: 'the',\n",
    " 5: 'examples',\n",
    " 6: 'first',\n",
    " 7: 'second',\n",
    " 8: 'do',\n",
    " 9: 'you',\n",
    " 10: 'want',\n",
    " 11: 'to',\n",
    " 12: 'see',\n",
    " 13: 'more',\n",
    " 14: 'or',\n",
    " 15: 'three',\n",
    " 16: 'enough'}\n",
    "\"\"\"\n",
    "\n",
    "## TF transform\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(B)\n",
    "Btf = tf_transformer.transform(B).todense()\n",
    "# Compute TF matrix \"manually\"\n",
    "# Btf[i,j] = L2-normalize(tf[i,:])_j\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "assert_allclose(Btf, normalize(B), atol=1e-2)\n",
    "assert_allclose(Btf, B / np.sqrt(np.sum(np.power(B, 2), axis=1)), atol=1e-2)\n",
    "\n",
    "\n",
    "## TF-IDF transform\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True)\n",
    "Btfidf = tfidf_transformer.fit_transform(B).todense()\n",
    "# Compute idf \"manually\"\n",
    "Bbin = B > 0  # Bbin[i,j]=1 iff word j occurs at least once in doc i\n",
    "df = np.ravel(np.sum(Bbin, axis=0))  # convert from (1,T) to (T)\n",
    "n = np.shape(B)[0]\n",
    "idf = np.log((1 + n) / (1 + df)) + 1\n",
    "assert_allclose(idf, tfidf_transformer.idf_, atol=1e-2)\n",
    "# Compute tf-idf \"manually\"\n",
    "tfidf = normalize(np.multiply(B, idf))\n",
    "assert_allclose(tfidf, Btfidf, atol=1e-2)\n",
    "\n",
    "\n",
    "# Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [(\"bow\", CountVectorizer(tokenizer=tokenizer)), (\"tfidf\", TfidfTransformer(use_idf=True, smooth_idf=True))]\n",
    ")\n",
    "Btrain = pipeline.fit_transform(corpus).todense()\n",
    "assert_allclose(Btfidf, Btrain)\n",
    "\n",
    "corpus_test = [\"This example is a new document.\", \"And this is the second test.\"]\n",
    "Btest = pipeline.transform(corpus_test)\n",
    "print(np.round(Btest.todense(), 3))\n",
    "\"\"\"\n",
    "[[0.    0.    0.62  0.    0.    0.481 0.    0.    0.    0.    0.    0.62\n",
    "  0.    0.    0.    0.   ]\n",
    " [0.    0.    0.    0.    0.    0.373 0.    0.    0.632 0.    0.48  0.48\n",
    "  0.    0.    0.    0.   ]]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
