{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ec009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature importance using tree ensembles. Based on the email spam example from chapter 10 of \"Elements of statistical learning\". Code is from Andrey Gaskov's site:\n",
    "\n",
    "#https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Spam.ipynb\n",
    "\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq pandas\n",
    "    import pandas as pd\n",
    "from matplotlib import transforms, pyplot as plt\n",
    "import numpy as np\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq scikit-learn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "# omit numpy warnings (don't do it in real work)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "np.warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "\n",
    "# define plots common properties and color constants\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "ORANGE, BLUE, PURPLE = '#FF8C00', '#0000FF', '#A020F0'\n",
    "GRAY1, GRAY4, GRAY7 = '#231F20', '#646369', '#929497'\n",
    "\n",
    "\n",
    "# we will calculate train and test error rates for all models\n",
    "def error_rate(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "\"\"\"Get data\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/data/Spam.txt?raw=True\")\n",
    "df.head()\n",
    "\n",
    "# PAGE 301. We coded spam as 1 and email as zero. A test set of size 1536 was\n",
    "#           randomly chosen, leaving 3065 observations in the training set.\n",
    "target = 'spam'\n",
    "columns = ['word_freq_make', 'word_freq_address', 'word_freq_all',\n",
    "           'word_freq_3d', 'word_freq_our', 'word_freq_over',\n",
    "           'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
    "           'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
    "           'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "           'word_freq_free', 'word_freq_business', 'word_freq_email',\n",
    "           'word_freq_you', 'word_freq_credit', 'word_freq_your',\n",
    "           'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
    "           'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
    "           'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "           'word_freq_telnet', 'word_freq_857', 'word_freq_data',\n",
    "           'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "           'word_freq_1999', 'word_freq_parts', 'word_freq_pm',\n",
    "           'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
    "           'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
    "           'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "           'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "           'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
    "           'capital_run_length_longest', 'capital_run_length_total']\n",
    "# let's give columns more compact names\n",
    "features = ['make', 'address', 'all', '3d', 'our', 'over', 'remove',\n",
    "            'internet', 'order', 'mail', 'receive', 'will', 'people',\n",
    "            'report', 'addresses', 'free', 'business', 'email', 'you',\n",
    "            'credit', 'your', 'font', '000', 'money', 'hp', 'hpl',\n",
    "            'george', '650', 'lab', 'labs', 'telnet', '857', 'data',\n",
    "            '415', '85', 'technology', '1999', 'parts', 'pm', 'direct',\n",
    "            'cs', 'meeting', 'original', 'project', 're', 'edu', 'table',\n",
    "            'conference', 'ch_;', 'ch(', 'ch[', 'ch!', 'ch$', 'ch#',\n",
    "            'CAPAVE', 'CAPMAX', 'CAPTOT']\n",
    "\n",
    "X, y = df[columns].values, df[target].values\n",
    "\n",
    "# split by test column value\n",
    "is_test = df.test.values\n",
    "X_train, X_test = X[is_test == 0], X[is_test == 1]\n",
    "y_train, y_test = y[is_test == 0], y[is_test == 1]\n",
    "\n",
    "\"\"\" Logistic regression\n",
    "\n",
    "As a sanity check, we try to match p301  test error rate of 7.6%.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq statsmodels\n",
    "    import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = sm.Logit(y_train, sm.add_constant(X_train)).fit(disp=False)\n",
    "# 0.5 is a threshold\n",
    "y_test_hat = (lr_clf.predict(sm.add_constant(X_test)) > 0.5).astype(int)\n",
    "lr_error_rate = error_rate(y_test, y_test_hat)\n",
    "print(f'Logistic Regression Test Error Rate: {lr_error_rate*100:.1f}%')\n",
    "\n",
    "\"\"\"Boosting\n",
    "\n",
    "p353 says gradient boosted trees (J=5 leaves) has test error is 4.5%.\n",
    "\"\"\"\n",
    "\n",
    "#!pip install -qq catboost\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq catboost\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "# find the best number of trees using 5 fold cross validation\n",
    "params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'iterations': 100,\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'random_seed': 100,\n",
    "    'learning_rate': 0.2}\n",
    "cv_data = cv(\n",
    "    params=params,\n",
    "    pool=Pool(X_train, label=y_train),\n",
    "    fold_count=5,\n",
    "    shuffle=True,\n",
    "    partition_random_seed=0,\n",
    "    stratified=True,\n",
    "    verbose=False\n",
    ")\n",
    "best_iter = np.argmax(cv_data['test-Accuracy-mean'].values)\n",
    "print(f'Best number of iterations {best_iter}')\n",
    "\n",
    "# PAGE 352. Applying gradient boosting to these data resulted in a test error\n",
    "#           rate of 4.5%, using the same test set as was used in Section 9.1.2.\n",
    "\n",
    "# refit model the the whole data using found the best number of trees\n",
    "cb_clf = CatBoostClassifier(\n",
    "    iterations=best_iter,\n",
    "    random_seed=100,\n",
    "    learning_rate=0.2\n",
    ").fit(X_train, y_train, verbose=False)\n",
    "cb_error_rate = error_rate(y_test, cb_clf.predict(X_test))\n",
    "print(f'Boosting Test Error Rate: {cb_error_rate*100:.1f}%')\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "def plot_relative_feature_importance(importance):\n",
    "    max_importance = np.max(importance)\n",
    "    relative_importance = sorted(zip(100*importance/max_importance, features))\n",
    "    yticks = np.arange(len(relative_importance))\n",
    "    yticklabels = [ri[1] for ri in relative_importance][::-1]\n",
    "    bars_sizes = [ri[0] for ri in relative_importance][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.3, 6.5), dpi=150)\n",
    "    bars = ax.barh(yticks, bars_sizes, height=0.8, color='red')\n",
    "    plt.setp(ax, yticks=yticks, yticklabels=yticklabels)\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([-0.5, 57])\n",
    "    for e in ax.get_yticklabels()+ax.get_xticklabels():\n",
    "        e.set_fontsize(6)\n",
    "        e.set_color(GRAY1)\n",
    "    ax.tick_params(left=False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    offset = transforms.ScaledTranslation(0, -0.07, fig.dpi_scale_trans)\n",
    "    for e in ax.get_xticklabels() + ax.xaxis.get_ticklines() + \\\n",
    "             [ax.spines['bottom']]:\n",
    "        e.set_transform(e.get_transform() + offset)\n",
    "    ax.spines['bottom'].set_bounds(0, 100)\n",
    "    _ = ax.set_xlabel('Relative Importance', color=GRAY4, fontsize=7)\n",
    "\n",
    "# PAGE 354. FIGURE 10.6. Predictor variable importance spectrum for the spam\n",
    "#           data. The variable names are written on the vertical axis.\n",
    "plot_relative_feature_importance(np.array(cb_clf.get_feature_importance()))\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/spam_feature_importance.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Partial dependence \n",
    "\n",
    "def plot_partial_dependence(ax, feature):\n",
    "    n = features.index(feature)\n",
    "    X_tmp = X.copy()\n",
    "    vals = np.unique(np.percentile(X_tmp[:, n], np.linspace(5, 95, 100)))\n",
    "    result = []\n",
    "    for i in range(vals.shape[0]):\n",
    "        X_tmp[:, n] = vals[i]\n",
    "        pr = np.mean(cb_clf.predict_proba(X_tmp), axis=0)\n",
    "        result.append(np.log(pr[1]/pr[0]))\n",
    "    #ax.plot(vals, result, linewidth=0.6, color='#26FF26')\n",
    "    ax.plot(vals, result, linewidth=1.5, color='#26FF26')\n",
    "    for e in ax.get_yticklabels() + ax.get_xticklabels():\n",
    "        e.set_fontsize(4)\n",
    "    ax.set_ylabel('Partial Dependance', color=GRAY4, fontsize=8)\n",
    "    ax.set_xlabel(f'{feature}', color=GRAY4, fontsize=8)\n",
    "    ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.15)\n",
    "\n",
    "    # plot small red lines for the data deciles\n",
    "    deciles = np.percentile(X[:, n], np.linspace(10, 90, 9))\n",
    "    y_from, y_to = ax.get_ylim()\n",
    "    for i in range(deciles.shape[0]):\n",
    "        x = deciles[i]\n",
    "        ax.plot([x, x], [y_from, y_from+(y_to-y_from)*0.05],\n",
    "                color='red', linewidth=1)\n",
    "    ax.set_ylim(y_from, y_to)\n",
    "\n",
    "# PAGE 355. FIGURE 10.7. Partial dependence of log-odds of spam on four\n",
    "#           important predictors. The red ticks at the base of the plots are\n",
    "#           deciles of the input variable.\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(4.65, 3.43), dpi=150)\n",
    "plt.subplots_adjust(wspace=0.35, hspace=0.45)\n",
    "plot_partial_dependence(axarr[0, 0], 'ch!')\n",
    "plot_partial_dependence(axarr[0, 1], 'remove')\n",
    "plot_partial_dependence(axarr[1, 0], 'edu')\n",
    "plot_partial_dependence(axarr[1, 1], 'hp')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/spam_partial_dependence.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# prepare colormap the looks similar to colormap from the book\n",
    "N = 256\n",
    "vals = np.ones((N, 4))\n",
    "vals[:, 0] = np.linspace(1, 0, N)\n",
    "vals[:, 1] = np.linspace(0, 1, N)\n",
    "vals[:, 2] = np.linspace(1, 1, N)\n",
    "newcmp = ListedColormap(vals)\n",
    "\n",
    "# calculate coordinates grids for surface and frame plotting\n",
    "n1, n2 = features.index('hp'), features.index('ch!')\n",
    "vals1, vals2 = np.linspace(0, 3, 40), np.linspace(0, 1, 20)\n",
    "N1, N2 = np.meshgrid(vals1, vals2)\n",
    "LO = np.zeros(shape=N1.shape)\n",
    "\n",
    "X_tmp = X.copy()\n",
    "for i in range(N1.shape[0]):\n",
    "    for j in range(N1.shape[1]):\n",
    "        X_tmp[:, n1], X_tmp[:, n2] = N1[i, j], N2[i, j]\n",
    "        pr = np.mean(cb_clf.predict_proba(X_tmp), axis=0)\n",
    "        LO[i, j] = np.log(pr[1]/pr[0])\n",
    "\n",
    "# PAGE 355. FIGURE 10.8. Partial dependence of the log-odds of spam vs. email\n",
    "#            as a function of joint frequences of hp and the character !.\n",
    "fig = plt.figure(figsize=(6, 3.75), dpi=150)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.set_xlabel('hp', fontsize=8)\n",
    "ax.set_ylabel('ch!', fontsize=8)\n",
    "ax.w_xaxis.line.set_color(GRAY7)\n",
    "ax.w_yaxis.line.set_color(GRAY7)\n",
    "ax.w_zaxis.line.set_color(GRAY7)\n",
    "ax.view_init(22, 81)\n",
    "# invert y-axis\n",
    "ax.set_ylim(1, 0)\n",
    "ax.set_xlim(0, 3)\n",
    "for e in ax.get_yticklabels() + ax.get_xticklabels() + \\\n",
    "         ax.get_zticklabels():\n",
    "    e.set_fontsize(7)\n",
    "\n",
    "ax.plot_surface(N1, N2, LO, cmap=newcmp, shade=False)\n",
    "_ = ax.plot_wireframe(N1, N2, LO, cmap=newcmp, linewidth=0.5, color='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/spam_joint_partial_dependence.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
