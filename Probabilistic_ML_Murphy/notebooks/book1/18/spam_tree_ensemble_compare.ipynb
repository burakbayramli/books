{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5db644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of tree ensembles. Based on the email spam example from chapter 10 of \"Elements of statistical learning\". Code is from Andrey Gaskov's site:\n",
    "\n",
    "# https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Spam.ipynb\n",
    "\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq pandas\n",
    "    import pandas as pd\n",
    "from matplotlib import transforms, pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq scikit-learn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "# omit numpy warnings (don't do it in real work)\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "np.warnings.filterwarnings(\"ignore\")\n",
    "# %matplotlib inline\n",
    "\n",
    "# define plots common properties and color constants\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.5\n",
    "ORANGE, BLUE, PURPLE = \"#FF8C00\", \"#0000FF\", \"#A020F0\"\n",
    "GRAY1, GRAY4, GRAY7 = \"#231F20\", \"#646369\", \"#929497\"\n",
    "\n",
    "\n",
    "# we will calculate train and test error rates for all models\n",
    "def error_rate(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\"\"\"Get data\"\"\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/data/Spam.txt?raw=True\"\n",
    ")\n",
    "df.head()\n",
    "\n",
    "# PAGE 301. We coded spam as 1 and email as zero. A test set of size 1536 was\n",
    "#           randomly chosen, leaving 3065 observations in the training set.\n",
    "target = \"spam\"\n",
    "columns = [\n",
    "    \"word_freq_make\",\n",
    "    \"word_freq_address\",\n",
    "    \"word_freq_all\",\n",
    "    \"word_freq_3d\",\n",
    "    \"word_freq_our\",\n",
    "    \"word_freq_over\",\n",
    "    \"word_freq_remove\",\n",
    "    \"word_freq_internet\",\n",
    "    \"word_freq_order\",\n",
    "    \"word_freq_mail\",\n",
    "    \"word_freq_receive\",\n",
    "    \"word_freq_will\",\n",
    "    \"word_freq_people\",\n",
    "    \"word_freq_report\",\n",
    "    \"word_freq_addresses\",\n",
    "    \"word_freq_free\",\n",
    "    \"word_freq_business\",\n",
    "    \"word_freq_email\",\n",
    "    \"word_freq_you\",\n",
    "    \"word_freq_credit\",\n",
    "    \"word_freq_your\",\n",
    "    \"word_freq_font\",\n",
    "    \"word_freq_000\",\n",
    "    \"word_freq_money\",\n",
    "    \"word_freq_hp\",\n",
    "    \"word_freq_hpl\",\n",
    "    \"word_freq_george\",\n",
    "    \"word_freq_650\",\n",
    "    \"word_freq_lab\",\n",
    "    \"word_freq_labs\",\n",
    "    \"word_freq_telnet\",\n",
    "    \"word_freq_857\",\n",
    "    \"word_freq_data\",\n",
    "    \"word_freq_415\",\n",
    "    \"word_freq_85\",\n",
    "    \"word_freq_technology\",\n",
    "    \"word_freq_1999\",\n",
    "    \"word_freq_parts\",\n",
    "    \"word_freq_pm\",\n",
    "    \"word_freq_direct\",\n",
    "    \"word_freq_cs\",\n",
    "    \"word_freq_meeting\",\n",
    "    \"word_freq_original\",\n",
    "    \"word_freq_project\",\n",
    "    \"word_freq_re\",\n",
    "    \"word_freq_edu\",\n",
    "    \"word_freq_table\",\n",
    "    \"word_freq_conference\",\n",
    "    \"char_freq_;\",\n",
    "    \"char_freq_(\",\n",
    "    \"char_freq_[\",\n",
    "    \"char_freq_!\",\n",
    "    \"char_freq_$\",\n",
    "    \"char_freq_#\",\n",
    "    \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\",\n",
    "]\n",
    "# let's give columns more compact names\n",
    "features = [\n",
    "    \"make\",\n",
    "    \"address\",\n",
    "    \"all\",\n",
    "    \"3d\",\n",
    "    \"our\",\n",
    "    \"over\",\n",
    "    \"remove\",\n",
    "    \"internet\",\n",
    "    \"order\",\n",
    "    \"mail\",\n",
    "    \"receive\",\n",
    "    \"will\",\n",
    "    \"people\",\n",
    "    \"report\",\n",
    "    \"addresses\",\n",
    "    \"free\",\n",
    "    \"business\",\n",
    "    \"email\",\n",
    "    \"you\",\n",
    "    \"credit\",\n",
    "    \"your\",\n",
    "    \"font\",\n",
    "    \"000\",\n",
    "    \"money\",\n",
    "    \"hp\",\n",
    "    \"hpl\",\n",
    "    \"george\",\n",
    "    \"650\",\n",
    "    \"lab\",\n",
    "    \"labs\",\n",
    "    \"telnet\",\n",
    "    \"857\",\n",
    "    \"data\",\n",
    "    \"415\",\n",
    "    \"85\",\n",
    "    \"technology\",\n",
    "    \"1999\",\n",
    "    \"parts\",\n",
    "    \"pm\",\n",
    "    \"direct\",\n",
    "    \"cs\",\n",
    "    \"meeting\",\n",
    "    \"original\",\n",
    "    \"project\",\n",
    "    \"re\",\n",
    "    \"edu\",\n",
    "    \"table\",\n",
    "    \"conference\",\n",
    "    \"ch_;\",\n",
    "    \"ch(\",\n",
    "    \"ch[\",\n",
    "    \"ch!\",\n",
    "    \"ch$\",\n",
    "    \"ch#\",\n",
    "    \"CAPAVE\",\n",
    "    \"CAPMAX\",\n",
    "    \"CAPTOT\",\n",
    "]\n",
    "\n",
    "X, y = df[columns].values, df[target].values\n",
    "\n",
    "# split by test column value\n",
    "is_test = df.test.values\n",
    "X_train, X_test = X[is_test == 0], X[is_test == 1]\n",
    "y_train, y_test = y[is_test == 0], y[is_test == 1]\n",
    "\n",
    "\"\"\" Logistic regression\n",
    "\n",
    "As a sanity check, we try to match p301  test error rate of 7.6%.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq statsmodels\n",
    "    import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = sm.Logit(y_train, sm.add_constant(X_train)).fit(disp=False)\n",
    "# 0.5 is a threshold\n",
    "y_test_hat = (lr_clf.predict(sm.add_constant(X_test)) > 0.5).astype(int)\n",
    "lr_error_rate = error_rate(y_test, y_test_hat)\n",
    "print(f\"Logistic Regression Test Error Rate: {lr_error_rate*100:.1f}%\")\n",
    "\n",
    "\n",
    "# PAGE 590. A random forest classifier achieves 4.88% misclassification error\n",
    "#           on the spam test data, which compares well with all other methods,\n",
    "#           and is not significantly worse than gradient boosting at 4.5%.\n",
    "ntrees_list = [10, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=ntrees, random_state=10).fit(X_train, y_train)\n",
    "    y_test_hat = rf_clf.predict(X_test)\n",
    "    rf_error_rate = error_rate(y_test, y_test_hat)\n",
    "    rf_errors.append(rf_error_rate)\n",
    "    print(f\"RF {ntrees} trees, test error {rf_error_rate*100:.1f}%\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq catboost\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "boost_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    boost_clf = CatBoostClassifier(iterations=ntrees, random_state=10, learning_rate=0.2, verbose=False).fit(\n",
    "        X_train, y_train\n",
    "    )\n",
    "    y_test_hat = boost_clf.predict(X_test)\n",
    "    boost_error_rate = error_rate(y_test, y_test_hat)\n",
    "    boost_errors.append(boost_error_rate)\n",
    "    print(f\"Boosting {ntrees} trees, test error {boost_error_rate*100:.1f}%\")\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    bag_clf = BaggingClassifier(n_estimators=ntrees, random_state=10, bootstrap=True).fit(X_train, y_train)\n",
    "    y_test_hat = bag_clf.predict(X_test)\n",
    "    bag_error_rate = error_rate(y_test, y_test_hat)\n",
    "    bag_errors.append(bag_error_rate)\n",
    "    print(f\"Bagged {ntrees} trees, test error {bag_error_rate*100:.1f}%\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ntrees_list, rf_errors, \"o-\", color=\"blue\", label=\"RF\")\n",
    "plt.plot(ntrees_list, boost_errors, \"x-\", color=\"green\", label=\"Boosting\")\n",
    "plt.plot(ntrees_list, bag_errors, \"^-\", color=\"orange\", label=\"Bagging\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Test error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/spam_tree_ensemble_compare.pdf\", dpi=300)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
