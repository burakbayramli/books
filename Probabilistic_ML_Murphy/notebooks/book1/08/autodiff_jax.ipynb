{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "autodiff_jax.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "DeXQ1pOcmhKu",
    "D2dSgG-cnVIe",
    "riXmT3Iok_yq",
    "QuMHSd3wr1xH",
    "kUj-VuSzmFaV",
    "VTIybB8b4ar0",
    "nglie5m7q607",
    "bBMcsg4uoKua",
    "mg9ValMRm_Md",
    "MeoGcnV54YY9",
    "EOzQJSX3JOF9",
    "ZF_OjIRrisDB",
    "AKTg9m9yz7Ib",
    "ZMZVMEOPbLWt",
    "TdtqCDWZAC2f",
    "A-L-dB_GBWNq",
    "YFsfzYHzhbM3",
    "QGxWcHWJKtsq",
    "o8iMrvUXK8Id",
    "8-wtZiZmU329",
    "Ru5D0tZEiV2e"
   ],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/supplements/autodiff_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4bE-S8yDALH"
   },
   "source": [
    "# Automatic differentiation using JAX \n",
    "\n",
    "In this section, we illustrate automatic differentation using JAX.\n",
    "For details, see see  [this video](https://www.youtube.com/watch?v=wG_nF1awSSY&t=697s)  or [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eCI0G3tfDFSs"
   },
   "source": [
    "# Standard Python libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from typing import Tuple, NamedTuple\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z9kAsUWYDIOk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4568e80c-e8a5-4a81-ecb0-20a2d994b0b1"
   },
   "source": [
    "# Load JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import random, vmap, jit, grad, value_and_grad, hessian, jacfwd, jacrev\n",
    "\n",
    "print(\"jax version {}\".format(jax.__version__))\n",
    "# Check the jax backend\n",
    "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
    "key = random.PRNGKey(0)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "jax version 0.2.9\n",
      "jax backend gpu\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuMHSd3wr1xH"
   },
   "source": [
    "## Derivatives\n",
    "\n",
    "We can compute $(\\nabla f)(x)$ using `grad(f)(x)`. For example, consider\n",
    "\n",
    "\n",
    "$f(x) = x^3 + 2x^2 - 3x + 1$\n",
    "\n",
    "$f'(x) = 3x^2 + 4x -3$\n",
    "\n",
    "$f''(x) = 6x + 4$\n",
    "\n",
    "$f'''(x) = 6$\n",
    "\n",
    "$f^{iv}(x) = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYTM5MPmr3C0",
    "outputId": "4801ef39-5c62-4fd1-c021-dc2d23e03035"
   },
   "source": [
    "f = lambda x: x**3 + 2 * x**2 - 3 * x + 1\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "d2fdx = jax.grad(dfdx)\n",
    "d3fdx = jax.grad(d2fdx)\n",
    "d4fdx = jax.grad(d3fdx)\n",
    "\n",
    "print(dfdx(1.0))\n",
    "print(d2fdx(1.0))\n",
    "print(d3fdx(1.0))\n",
    "print(d4fdx(1.0))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "4.0\n",
      "10.0\n",
      "6.0\n",
      "0.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUj-VuSzmFaV"
   },
   "source": [
    "## Partial derivatives\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) &= x^2 + y \\\\\n",
    "\\frac{\\partial f}{\\partial x} &= 2x \\\\\n",
    "\\frac{\\partial f}{\\partial y} &= 1 \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0hW7fqfmR1c",
    "outputId": "3b5bbc43-a7e3-4692-c4e8-137faa0c68eb"
   },
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y\n",
    "\n",
    "\n",
    "# Partial derviatives\n",
    "x = 2.0\n",
    "y = 3.0\n",
    "v, gx = value_and_grad(f, argnums=0)(x, y)\n",
    "print(v)\n",
    "print(gx)\n",
    "\n",
    "gy = grad(f, argnums=1)(x, y)\n",
    "print(gy)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "7.0\n",
      "4.0\n",
      "1.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTIybB8b4ar0"
   },
   "source": [
    "## Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb0gZ_1HBEyC"
   },
   "source": [
    "Linear function: multi-input, scalar output.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x; a) &= a^T x\\\\\n",
    "\\nabla_x f(x;a) &= a\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmYqFEs04vkV",
    "outputId": "311e8ac4-3ed4-4ad6-f545-6307390d4745"
   },
   "source": [
    "def fun1d(x):\n",
    "    return jnp.dot(a, x)[0]\n",
    "\n",
    "\n",
    "Din = 3\n",
    "Dout = 1\n",
    "a = np.random.normal(size=(Dout, Din))\n",
    "x = np.random.normal(size=(Din,))\n",
    "\n",
    "g = grad(fun1d)(x)\n",
    "assert np.allclose(g, a)\n",
    "\n",
    "\n",
    "# It is often useful to get the function value and gradient at the same time\n",
    "val_grad_fn = jax.value_and_grad(fun1d)\n",
    "v, g = val_grad_fn(x)\n",
    "print(v)\n",
    "print(g)\n",
    "assert np.allclose(v, fun1d(x))\n",
    "assert np.allclose(a, g)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "-1.0599848\n",
      "[-1.311  0.546  0.915]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbgiqkF6BL1E"
   },
   "source": [
    "Linear function: multi-input, multi-output.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x;A) &= A x \\\\\n",
    "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s6hkEYxV5EIx"
   },
   "source": [
    "# We construct a multi-output linear function.\n",
    "# We check forward and reverse mode give same Jacobians.\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return jnp.dot(A, x)\n",
    "\n",
    "\n",
    "Din = 3\n",
    "Dout = 4\n",
    "A = np.random.normal(size=(Dout, Din))\n",
    "x = np.random.normal(size=(Din,))\n",
    "Jf = jacfwd(fun)(x)\n",
    "Jr = jacrev(fun)(x)\n",
    "assert np.allclose(Jf, Jr)\n",
    "assert np.allclose(Jf, A)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN5d-D7XBU9Y"
   },
   "source": [
    "Quadratic form.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x;A) &= x^T A x \\\\\n",
    "\\nabla_x f(x;A) &= (A+A^T) x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9URZeX8PBbhl"
   },
   "source": [
    "D = 4\n",
    "A = np.random.normal(size=(D, D))\n",
    "x = np.random.normal(size=(D,))\n",
    "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
    "\n",
    "g = grad(quadfun)(x)\n",
    "assert np.allclose(g, jnp.dot(A + A.T, x))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ZOhDeqCXu3"
   },
   "source": [
    "Chain rule applied to sigmoid function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu(x;w) &=\\sigma(w^T x) \\\\\n",
    "\\nabla_w \\mu(x;w) &= \\sigma'(w^T x) x \\\\\n",
    "\\sigma'(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6q5VfLXLB7rv",
    "outputId": "0773a46f-784f-4dbe-a6b7-29dd5cd26654"
   },
   "source": [
    "D = 4\n",
    "w = np.random.normal(size=(D,))\n",
    "x = np.random.normal(size=(D,))\n",
    "y = 0\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2.0) + 1)\n",
    "\n",
    "\n",
    "def mu(w):\n",
    "    return sigmoid(jnp.dot(w, x))\n",
    "\n",
    "\n",
    "def deriv_mu(w):\n",
    "    return mu(w) * (1 - mu(w)) * x\n",
    "\n",
    "\n",
    "deriv_mu_jax = grad(mu)\n",
    "\n",
    "print(deriv_mu(w))\n",
    "print(deriv_mu_jax(w))\n",
    "\n",
    "assert np.allclose(deriv_mu(w), deriv_mu_jax(w), atol=1e-3)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[-0.458  0.022 -0.266 -0.005]\n",
      "[-0.458  0.022 -0.266 -0.005]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nglie5m7q607"
   },
   "source": [
    "## Auxiliary return values\n",
    "\n",
    "A function can return its value and other auxiliary results; the latter are not differentiated. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHz6zrC9qVjT",
    "outputId": "76724fd1-4e1a-4737-b252-963700fcce91"
   },
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y, 42\n",
    "\n",
    "\n",
    "(v, aux), g = value_and_grad(f, has_aux=True)(x, y)\n",
    "print(v)\n",
    "print(aux)\n",
    "print(g)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "7.0\n",
      "42\n",
      "4.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBMcsg4uoKua"
   },
   "source": [
    "## Jacobians\n",
    "\n",
    "\n",
    "Example: Linear function: multi-input, multi-output.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x;A) &= A x \\\\\n",
    "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iPilm5H3oWcy"
   },
   "source": [
    "# We construct a multi-output linear function.\n",
    "# We check forward and reverse mode give same Jacobians.\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return jnp.dot(A, x)\n",
    "\n",
    "\n",
    "Din = 3\n",
    "Dout = 4\n",
    "A = np.random.normal(size=(Dout, Din))\n",
    "x = np.random.normal(size=(Din,))\n",
    "Jf = jacfwd(fun)(x)\n",
    "Jr = jacrev(fun)(x)\n",
    "assert np.allclose(Jf, Jr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg9ValMRm_Md"
   },
   "source": [
    "## Hessians\n",
    "\n",
    "Quadratic form.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x;A) &= x^T A x \\\\\n",
    "\\nabla_x^2 f(x;A) &= A + A^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "leW9lqvinDsM"
   },
   "source": [
    "D = 4\n",
    "A = np.random.normal(size=(D, D))\n",
    "x = np.random.normal(size=(D,))\n",
    "\n",
    "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
    "\n",
    "\n",
    "H1 = hessian(quadfun)(x)\n",
    "assert np.allclose(H1, A + A.T)\n",
    "\n",
    "\n",
    "def my_hessian(fun):\n",
    "    return jacfwd(jacrev(fun))\n",
    "\n",
    "\n",
    "H2 = my_hessian(quadfun)(x)\n",
    "assert np.allclose(H1, H2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeoGcnV54YY9"
   },
   "source": [
    "## Example: Binary logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Isql2l4MGfIt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2465d977-66dc-4ea0-e5bd-5a05d4ad92ec"
   },
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2.0) + 1)\n",
    "\n",
    "\n",
    "def predict_single(w, x):\n",
    "    return sigmoid(jnp.dot(w, x))  # <(D) , (D)> = (1) # inner product\n",
    "\n",
    "\n",
    "def predict_batch(w, X):\n",
    "    return sigmoid(jnp.dot(X, w))  # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
    "\n",
    "\n",
    "# negative log likelihood\n",
    "def loss(weights, inputs, targets):\n",
    "    preds = predict_batch(weights, inputs)\n",
    "    logprobs = jnp.log(preds) * targets + jnp.log(1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(logprobs)\n",
    "\n",
    "\n",
    "D = 2\n",
    "N = 3\n",
    "w = jax.random.normal(key, shape=(D,))\n",
    "X = jax.random.normal(key, shape=(N, D))\n",
    "y = jax.random.choice(key, 2, shape=(N,))  # uniform binary labels\n",
    "# logits = jnp.dot(X, w)\n",
    "# y = jax.random.categorical(key, logits)\n",
    "\n",
    "print(loss(w, X, y))\n",
    "\n",
    "# Gradient function\n",
    "grad_fun = grad(loss)\n",
    "\n",
    "# Gradient of each example in the batch - 2 different ways\n",
    "grad_fun_w = partial(grad_fun, w)\n",
    "grads = vmap(grad_fun_w)(X, y)\n",
    "print(grads)\n",
    "assert grads.shape == (N, D)\n",
    "\n",
    "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y)\n",
    "assert np.allclose(grads, grads2)\n",
    "\n",
    "# Gradient for entire batch\n",
    "grad_sum = jnp.sum(grads, axis=0)\n",
    "assert grad_sum.shape == (D,)\n",
    "print(grad_sum)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "1.5545294\n",
      "[[ 0.042 -0.287]\n",
      " [-0.236 -0.454]\n",
      " [-0.14   0.067]]\n",
      "[-0.334 -0.673]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3BaHdT4Gj6W",
    "outputId": "12dc846e-dd7e-4907-ea76-73147ea6f77f"
   },
   "source": [
    "# Textbook implementation of gradient\n",
    "def NLL_grad(weights, batch):\n",
    "    X, y = batch\n",
    "    N = X.shape[0]\n",
    "    mu = predict_batch(weights, X)\n",
    "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)\n",
    "    return g\n",
    "\n",
    "\n",
    "grad_sum_batch = NLL_grad(w, (X, y))\n",
    "print(grad_sum_batch)\n",
    "assert np.allclose(grad_sum, grad_sum_batch)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[-0.334 -0.673]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_4lRrHgpLbG",
    "outputId": "d47db27a-b171-4933-de08-58bc5bbe0c2f"
   },
   "source": [
    "# We can also compute Hessians, as we illustrate below.\n",
    "\n",
    "hessian_fun = hessian(loss)\n",
    "\n",
    "# Hessian on one example\n",
    "H0 = hessian_fun(w, X[0, :], y[0])\n",
    "print(\"Hessian(example 0)\\n{}\".format(H0))\n",
    "\n",
    "# Hessian for batch\n",
    "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y)\n",
    "print(\"Hbatch shape {}\".format(Hbatch.shape))\n",
    "\n",
    "Hbatch_sum = jnp.sum(Hbatch, axis=0)\n",
    "print(\"Hbatch sum\\n {}\".format(Hbatch_sum))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Hessian(example 0)\n",
      "[[ 0.006 -0.042]\n",
      " [-0.042  0.286]]\n",
      "Hbatch shape (3, 2, 2)\n",
      "Hbatch sum\n",
      " [[0.118 0.139]\n",
      " [0.139 0.65 ]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QcJvgukUpWWE"
   },
   "source": [
    "# Textbook implementation of Hessian\n",
    "\n",
    "\n",
    "def NLL_hessian(weights, batch):\n",
    "    X, y = batch\n",
    "    mu = predict_batch(weights, X)\n",
    "    S = jnp.diag(mu * (1 - mu))\n",
    "    H = jnp.dot(jnp.dot(X.T, S), X)\n",
    "    return H\n",
    "\n",
    "\n",
    "H2 = NLL_hessian(w, (X, y))\n",
    "\n",
    "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOzQJSX3JOF9"
   },
   "source": [
    "## Vector Jacobian Products (VJP) and Jacobian Vector Products (JVP)\n",
    "\n",
    "Consider a bilinear mapping $f(x,W) = x W$.\n",
    "For fixed parameters, we have\n",
    "$f1(x) = W x$, so $J(x) = W$, and $u^T J(x) = J(x)^T u = W^T u$.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Lpmntn2JREG",
    "outputId": "39778872-aca9-488c-f9dc-40d30a821488"
   },
   "source": [
    "n = 3\n",
    "m = 2\n",
    "W = jax.random.normal(key, shape=(m, n))\n",
    "x = jax.random.normal(key, shape=(n,))\n",
    "u = jax.random.normal(key, shape=(m,))\n",
    "\n",
    "\n",
    "def f1(x):\n",
    "    return jnp.dot(W, x)\n",
    "\n",
    "\n",
    "J1 = jacfwd(f1)(x)\n",
    "print(J1.shape)\n",
    "\n",
    "assert np.allclose(J1, W)\n",
    "tmp1 = jnp.dot(u.T, J1)\n",
    "print(tmp1)\n",
    "\n",
    "(val, jvp_fun) = jax.vjp(f1, x)\n",
    "\n",
    "tmp2 = jvp_fun(u)\n",
    "\n",
    "assert np.allclose(tmp1, tmp2)\n",
    "\n",
    "tmp3 = np.dot(W.T, u)\n",
    "assert np.allclose(tmp1, tmp3)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[ 0.922  1.216 -0.61 ]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYSC6DMOO3IS"
   },
   "source": [
    "For fixed inputs, we have\n",
    "$f2(W) = W x$, so $J(W) = \\text{something complex}$,\n",
    "but $u^T J(W) = J(W)^T u = u x^T$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8l3StJdO1r1",
    "outputId": "5a3f1e5b-b903-4db8-bb8a-a53c29bcad19"
   },
   "source": [
    "def f2(W):\n",
    "    return jnp.dot(W, x)\n",
    "\n",
    "\n",
    "J2 = jacfwd(f2)(W)\n",
    "print(J2.shape)\n",
    "\n",
    "tmp1 = jnp.dot(u.T, J2)\n",
    "print(tmp1)\n",
    "print(tmp1.shape)\n",
    "\n",
    "(val, jvp_fun) = jax.vjp(f2, W)\n",
    "tmp2 = jvp_fun(u)\n",
    "assert np.allclose(tmp1, tmp2)\n",
    "\n",
    "tmp3 = np.outer(u, x)\n",
    "assert np.allclose(tmp1, tmp3)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[-1.425  0.379 -0.267]\n",
      " [ 1.555 -0.413  0.291]]\n",
      "(2, 3)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF_OjIRrisDB"
   },
   "source": [
    "## Stop-gradient\n",
    "\n",
    "Sometimes we want to take the gradient of a complex expression wrt some parameters $\\theta$, but treating $\\theta$ as a constant for some parts of the expression. For example, consider the TD(0) update in reinforcement learning, which as the following form:\n",
    "\n",
    "\n",
    "$\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})$\n",
    "\n",
    "where $s$ is the state, $r$ is the reward, and $v$ is the value function.\n",
    "This update is not the gradient of any loss function.\n",
    "However it can be **written** as the gradient of the pseudo loss function\n",
    "\n",
    "$L(\\theta) = [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2$\n",
    "\n",
    "since\n",
    "\n",
    "$\\nabla_{\\theta} L(\\theta) = 2 [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})] \\nabla v_{\\theta}(s_{t-1})$\n",
    "\n",
    "if the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored. We can implement this in JAX using `stop_gradient`, as we show below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9cprxb9jbsK",
    "outputId": "b345e287-b8fa-417f-d20e-0f1951e1f64e"
   },
   "source": [
    "def td_loss(theta, s_prev, r_t, s_t):\n",
    "    v_prev = value_fn(theta, s_prev)\n",
    "    target = r_t + value_fn(theta, s_t)\n",
    "    return 0.5 * (jax.lax.stop_gradient(target) - v_prev) ** 2\n",
    "\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "\n",
    "# An example transition.\n",
    "s_prev = jnp.array([1.0, 2.0, -1.0])\n",
    "r_t = jnp.array(1.0)\n",
    "s_t = jnp.array([2.0, 1.0, 0.0])\n",
    "\n",
    "# Value function and initial parameters\n",
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.0])\n",
    "\n",
    "print(td_update(theta, s_prev, r_t, s_t))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[-1.2 -2.4  1.2]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKTg9m9yz7Ib"
   },
   "source": [
    "## Straight through estimator\n",
    "\n",
    "The straight-through estimator is a trick for defining a 'gradient' of a function that is otherwise non-differentiable. Given a non-differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that $f$ is the identity function, so gradients pass through $f$ ignoring the $f'$ term. This can be implemented neatly using `jax.lax.stop_gradient`.\n",
    "\n",
    "Here is an example of a non-differentiable function that converts a soft probability distribution to a one-hot vector (discretization).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIFVQKrwqAG4",
    "outputId": "67ada739-47e3-4ef9-c11e-76a64862ea80"
   },
   "source": [
    "def onehot(labels, num_classes):\n",
    "    y = labels[..., None] == jnp.arange(num_classes)[None]\n",
    "    return y.astype(jnp.float32)\n",
    "\n",
    "\n",
    "def quantize(y_soft):\n",
    "    y_hard = onehot(jnp.argmax(y_soft), 3)[0]\n",
    "    return y_hard\n",
    "\n",
    "\n",
    "y_soft = np.array([0.1, 0.2, 0.7])\n",
    "print(quantize(y_soft))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSuQMr61sg16"
   },
   "source": [
    "Now suppose we define some linear function of the quantized variable of the form $f(y) = w^T q(y)$. If $w=[1,2,3]$ and $q(y)=[0,0,1]$, we get $f(y) = 3$. But the gradient is 0 because $q$ is not differentiable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDEMPSdJsTpl",
    "outputId": "0f329a87-2271-47d2-f23b-be47ed79a618"
   },
   "source": [
    "def f(y):\n",
    "    w = jnp.array([1, 2, 3])\n",
    "    yq = quantize(y)\n",
    "    return jnp.dot(w, yq)\n",
    "\n",
    "\n",
    "print(f(y_soft))\n",
    "print(grad(f)(y_soft))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[0. 0. 0.]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTcVC08Rs4DM"
   },
   "source": [
    "To use the straight-through estimator, we replace $q(y)$ with \n",
    "$$y + SG(q(y)-y)$$, where SG is stop gradient. In the forwards pass, we have $y+q(y)-y=q(y)$. In the backwards pass, the gradient of SG is 0, so we effectively replace $q(y)$ with $y$. So in the backwarsd pass we have\n",
    "$$\n",
    "\\begin{align}\n",
    "f(y) &= w^T q(y) \\approx w^T  y \\\\\n",
    "\\nabla_y f(y) &\\approx w\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5m_k7Ju3sUVq",
    "outputId": "cfcd7769-de40-4fc7-d29c-19d6a7d04db0"
   },
   "source": [
    "def f_ste(y):\n",
    "    w = jnp.array([1, 2, 3])\n",
    "    yq = quantize(y)\n",
    "    yy = y + jax.lax.stop_gradient(yq - y)  # gives yq on fwd, and y on backward\n",
    "    return jnp.dot(w, yy)\n",
    "\n",
    "\n",
    "print(f_ste(y_soft))\n",
    "print(grad(f_ste)(y_soft))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[1. 2. 3.]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwMIYqiOp0U0"
   },
   "source": [
    "## Per-example gradients\n",
    "\n",
    "In some applications, we want to compute the gradient for every example in a batch, not just the sum of gradients over the batch. This is hard in other frameworks like TF and PyTorch but easy in JAX, as we show below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKhNPiq4qGhl",
    "outputId": "0102b6ff-1fc1-4870-9f58-1b4e7db92ba9"
   },
   "source": [
    "def loss(w, x):\n",
    "    return jnp.dot(w, x)\n",
    "\n",
    "\n",
    "w = jnp.ones((3,))\n",
    "x0 = jnp.array([1.0, 2.0, 3.0])\n",
    "x1 = 2 * x0\n",
    "X = jnp.stack([x0, x1])\n",
    "print(X.shape)\n",
    "\n",
    "perex_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0)))\n",
    "print(perex_grads(w, X))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[1. 2. 3.]\n",
      " [2. 4. 6.]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxPeXpUaq-_p"
   },
   "source": [
    "To explain the above code in more depth, note that the vmap converts the function loss to take  a batch of inputs for each of its arguments, and returns a batch of outputs. To make it work with a single weight vector, we specify in_axes=(None,0), meaning the first argument (w) is not replicated, and the second argument (x) is replicated along dimension 0. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYB1L8JKrVKo",
    "outputId": "b78026f2-7ff2-4228-be72-cd0c1b281b2b"
   },
   "source": [
    "gradfn = jax.grad(loss)\n",
    "\n",
    "W = jnp.stack([w, w])\n",
    "print(jax.vmap(gradfn)(W, X))\n",
    "\n",
    "print(jax.vmap(gradfn, in_axes=(None, 0))(w, X))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [2. 4. 6.]]\n",
      "[[1. 2. 3.]\n",
      " [2. 4. 6.]]\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}