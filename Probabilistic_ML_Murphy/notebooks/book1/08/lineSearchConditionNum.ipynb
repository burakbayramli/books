{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfa775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize, line_search\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap, jit, grad, value_and_grad, hessian, jacfwd, jacrev\n",
    "\n",
    "# Objective is a quadratic\n",
    "# f(x) = 0.5 x'Ax + b'x + c\n",
    "\n",
    "\n",
    "def gradient_descent(x0, f, f_prime, hessian, stepsize=None, nsteps=50):\n",
    "    \"\"\"\n",
    "    Steepest-Descent algorithm with option for line search\n",
    "    \"\"\"\n",
    "    x_i, y_i = x0\n",
    "    all_x_i = list()\n",
    "    all_y_i = list()\n",
    "    all_f_i = list()\n",
    "\n",
    "    for i in range(1, nsteps):\n",
    "        all_x_i.append(x_i)\n",
    "        all_y_i.append(y_i)\n",
    "        x = np.array([x_i, y_i])\n",
    "        all_f_i.append(f(x))\n",
    "        dx_i, dy_i = f_prime(x)\n",
    "        if stepsize is None:\n",
    "            # Compute a step size using a line_search to satisfy the Wolf\n",
    "            # conditions\n",
    "            step = line_search(f, f_prime, np.r_[x_i, y_i], -np.r_[dx_i, dy_i], np.r_[dx_i, dy_i], c2=0.05)\n",
    "            step = step[0]\n",
    "            if step is None:\n",
    "                step = 0\n",
    "        else:\n",
    "            step = stepsize\n",
    "        x_i += -step * dx_i\n",
    "        y_i += -step * dy_i\n",
    "        if np.abs(all_f_i[-1]) < 1e-5:\n",
    "            break\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def make_plot(A, b, c, fname):\n",
    "    def objective(x):  # x is (2,)\n",
    "        f = jnp.dot(x, jnp.dot(A, x)) + jnp.dot(x, b) + c\n",
    "        return f\n",
    "\n",
    "    def objective_vectorized(X):  # x is (N,2)\n",
    "        f = vmap(objective)(X)\n",
    "        return f\n",
    "\n",
    "    def gradient(x):\n",
    "        return jnp.dot(A + A.T, x) + b\n",
    "\n",
    "    def hessian(x):\n",
    "        return A\n",
    "\n",
    "    z = objective_vectorized(X)\n",
    "    N = len(x1)\n",
    "    z = np.reshape(z, (N, N))\n",
    "    plt.contour(x1, x2, z, 50)\n",
    "    x0 = np.array((0.0, 0.0))\n",
    "    # x0 = np.array((-1.0, -1.0))\n",
    "    xs, ys, fs = gradient_descent(x0, objective, gradient, hessian, stepsize=None)\n",
    "    nsteps = 20\n",
    "    plt.scatter(xs[:nsteps], ys[:nsteps])\n",
    "    plt.plot(xs[:nsteps], ys[:nsteps])\n",
    "    plt.title(\"condition number of A={:0.3f}\".format(np.linalg.cond(A)))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/{}.pdf\".format(fname), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "x1 = np.arange(-2, 2, 0.1)  # xs is (40,)\n",
    "x2 = x1\n",
    "xs, ys = np.meshgrid(x1, x2)  # xs is (1600,)\n",
    "X = np.stack((xs.flatten(), ys.flatten()), axis=1)  # (1600,2)\n",
    "\n",
    "A = np.array([[20, 5], [5, 2]])\n",
    "b = np.array([-14, -6])\n",
    "c = 10\n",
    "fname = \"steepestDescentCondNumBig\"\n",
    "make_plot(A, b, c, fname)\n",
    "\n",
    "A = np.array([[20, 5], [5, 16]])\n",
    "fname = \"steepestDescentCondNumSmall\"\n",
    "make_plot(A, b, c, fname)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
