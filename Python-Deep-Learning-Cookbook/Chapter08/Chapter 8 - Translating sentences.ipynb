{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT\n",
    "from cntk.learners import momentum_sgd, fsadagrad, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk import input, cross_entropy_with_softmax, classification_error, sequence, element_select, alias, hardmax, placeholder, combine, parameter, times, plus\n",
    "from cntk.ops.functions import CloneMethod, load_model, Function\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.logging import log_number_of_parameters, ProgressPrinter\n",
    "from cntk.logging.graph import plot\n",
    "from cntk.layers import *\n",
    "from cntk.layers.sequence import *\n",
    "from cntk.layers.models.attention import *\n",
    "from cntk.layers.typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eng = 'Data/translations/small_vocab_en'\n",
    "data_fr = 'Data/translations/small_vocab_fr'\n",
    "'../Data/tr'\n",
    "\n",
    "with open(data_eng, 'r', encoding='utf-8') as f:\n",
    "    sentences_eng = f.read()\n",
    "    \n",
    "with open(data_fr, 'r', encoding='utf-8') as f:\n",
    "    sentences_fr = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [len(sentence.split()) for sentence in sentences_eng]\n",
    "print('Number of unique words in English: {}'.format(len({word: None for word in sentences_eng.lower().split()})))\n",
    "print('Number of sentences: {}'.format(len(sentences_eng)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "n_examples = 5\n",
    "for i in range(n_examples):\n",
    "    print('\\nExample {}'.format(i))\n",
    "    print(sentences_eng.split('\\n')[i])\n",
    "    print(sentences_fr.split('\\n')[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = {'<S>': 0, '<E>': 1, '<UNK>': 2, '<PAD>': 3 }\n",
    "\n",
    "    for i, v in enumerate(vocab, len(vocab_to_int)):\n",
    "        vocab_to_int[v] = i\n",
    "\n",
    "    int_to_vocab = {i: v for v, i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "vocab_to_int_eng, int_to_vocab_eng = create_lookup_tables(sentences_eng.lower())\n",
    "vocab_to_int_fr, int_to_vocab_fr = create_lookup_tables(sentences_fr.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sentence.split()] for sentence in source_text.split('\\n')]\n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sentence.split()]+[target_vocab_to_int['<E>']] for sentence in target_text.split('\\n')]\n",
    "\n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "X, y = text_to_ids(sentences_eng.lower(), sentences_fr.lower(), vocab_to_int_eng, vocab_to_int_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_dim = 128\n",
    "label_vocab_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "attention_dim = 128\n",
    "attention_span = 12\n",
    "embedding_dim = 200\n",
    "n_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_layers):\n",
    "    embed = Embedding(embedding_dim, name='embed')\n",
    "    \n",
    "    LastRecurrence = C.layers.Recurrence\n",
    "    encode = C.layers.Sequential([\n",
    "        embed,\n",
    "        C.layers.Stabilizer(),\n",
    "        C.layers.For(range(num_layers-1), lambda:\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim))),\n",
    "        LastRecurrence(C.layers.LSTM(hidden_dim), return_full_state=True),\n",
    "        (C.layers.Label('encoded_h'), C.layers.Label('encoded_c')),\n",
    "    ])\n",
    "    \n",
    "    with default_options(enable_self_stabilization=True):\n",
    "        stab_in = Stabilizer()\n",
    "        rec_blocks = [LSTM(hidden_dim) for i in range(n_layers)]\n",
    "        stab_out = Stabilizer()\n",
    "        out = Dense(label_vocab_dim, name='out')\n",
    "        attention_model = AttentionModel(attention_dim, None, None, name='attention_model')\n",
    "\n",
    "        @Function\n",
    "        def decode(history, input):\n",
    "            encoded_input = encode(input)\n",
    "            r = history\n",
    "            r = embed(r)\n",
    "            r = stab_in(r)\n",
    "            for i in range(n_layers):\n",
    "                rec_block = rec_blocks[i]\n",
    "                @Function\n",
    "                def lstm_with_attention(dh, dc, x):\n",
    "                    h_att = attention_model(encoded_input.outputs[0], dh)\n",
    "                    x = splice(x, h_att)\n",
    "                    return rec_block(dh, dc, x)\n",
    "                r = Recurrence(lstm_with_attention)(r)\n",
    "            r = stab_out(r)\n",
    "            r = out(r)\n",
    "            r = Label('out')(r)\n",
    "            return r\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_function(model):\n",
    "    @Function\n",
    "    @Signature(input = InputSequence[Tensor[input_vocab_dim]], labels = LabelSequence[Tensor[label_vocab_dim]])\n",
    "    def loss (input, labels):\n",
    "        postprocessed_labels = sequence.slice(labels, 1, 0)\n",
    "        z = model(input, postprocessed_labels)\n",
    "        ce = cross_entropy_with_softmax(z, postprocessed_labels)\n",
    "        errs = classification_error (z, postprocessed_labels)\n",
    "        return (ce, errs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    @Function\n",
    "    def model_train(input, labels):\n",
    "        past_labels = Delay(initial_state=sentence_start)(labels)\n",
    "        return s2smodel(past_labels, input)\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_reader, valid_reader, vocab, i2w, s2smodel, max_epochs, epoch_size):\n",
    "    model_train = create_model_train(s2smodel)\n",
    "    loss = create_loss_function(model_train)\n",
    "    learner = fsadagrad(model_train.parameters,\n",
    "                        lr = learning_rate,\n",
    "                        momentum = momentum_as_time_constant_schedule(1100),\n",
    "                        gradient_clipping_threshold_per_sample=2.3,\n",
    "                        gradient_clipping_with_truncation=True)\n",
    "    trainer = Trainer(None, loss, learner)\n",
    "\n",
    "    total_samples = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        while total_samples < (epoch+1) * epoch_size:\n",
    "            mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "            #trainer.train_minibatch(mb_train[train_reader.streams.features], mb_train[train_reader.streams.labels])\n",
    "            trainer.train_minibatch({criterion.arguments[0]: mb_train[train_reader.streams.features], criterion.arguments[1]: mb_train[train_reader.streams.labels]})\n",
    "            total_samples += mb_train[train_reader.streams.labels].num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features = StreamDef(field='S0', shape=input_vocab_dim, is_sparse=True),\n",
    "        labels   = StreamDef(field='S1', shape=label_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(2)\n",
    "loss = create_loss_function(model)\n",
    "learner = fsadagrad(model.parameters,\n",
    "                    lr = learning_rate,\n",
    "                    momentum = momentum_as_time_constant_schedule(1100),\n",
    "                    gradient_clipping_threshold_per_sample=2.3,\n",
    "                    gradient_clipping_with_truncation=True)\n",
    "trainer = Trainer(None, loss, learner)\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    while total_samples < (epoch+1) * epoch_size:\n",
    "        mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "        trainer.train_minibatch(mb_train[train_reader.streams.features], mb_train[train_reader.streams.labels])\n",
    "        total_samples += mb_train[train_reader.streams.labels].num_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
