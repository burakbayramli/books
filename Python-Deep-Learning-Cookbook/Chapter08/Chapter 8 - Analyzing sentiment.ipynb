{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np \n",
    "import random \n",
    "import pickle \n",
    "from collections import Counter \n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "max_lines = 10000000 \n",
    "pos = 'Data/sentences/positive.txt'\n",
    "neg = 'Data/sentences/negative.txt'\n",
    "\n",
    "def create_lexicon(pos, neg): \n",
    "    lexicon = [] \n",
    "    for fi in [pos, neg]: \n",
    "        with open(fi, 'r') as f: \n",
    "            contents = f.readlines() \n",
    "            for l in contents[:max_lines]: \n",
    "                all_words = word_tokenize(l.lower()) \n",
    "                lexicon += list(all_words) \n",
    " \n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon] \n",
    "    w_counts = Counter(lexicon) \n",
    " \n",
    "    l2 =[] \n",
    "    for w in w_counts: \n",
    "        if 1000 > w_counts[w] > 50: \n",
    "            l2.append(w) \n",
    "    return l2 \n",
    " \n",
    "def sample_handling(sample,lexicon,classification): \n",
    "    featureset = [] \n",
    "    with open(sample,'r') as f: \n",
    "        contents = f.readlines() \n",
    "        for l in contents[:max_lines]: \n",
    "            current_words = word_tokenize(l.lower()) \n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words] \n",
    "            features = np.zeros(len(lexicon)) \n",
    "            for word in current_words: \n",
    "                if word.lower() in lexicon: \n",
    "                    index_value = lexicon.index(word.lower()) \n",
    "                    features[index_value] += 1 \n",
    " \n",
    "            features = list(features) \n",
    "            featureset.append([features,classification]) \n",
    " \n",
    "    return featureset \n",
    " \n",
    "lexicon = create_lexicon(pos,neg) \n",
    "features = [] \n",
    "features += sample_handling(pos, lexicon,[1,0]) \n",
    "features += sample_handling(neg, lexicon,[0,1]) \n",
    "random.shuffle(features) \n",
    "features = np.array(features) \n",
    "\n",
    "testing_size = int(0.1*len(features)) \n",
    "\n",
    "X_train = list(features[:,0][:-testing_size]) \n",
    "y_train = list(features[:,1][:-testing_size]) \n",
    "X_test = list(features[:,0][-testing_size:]) \n",
    "y_test = list(features[:,1][-testing_size:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10 \n",
    "batch_size = 128 \n",
    "h1 = 500 \n",
    "h2 = 500  \n",
    "n_classes = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder('float') \n",
    "y_input = tf.placeholder('float') \n",
    " \n",
    "hidden_1 = {'weight':tf.Variable(tf.random_normal([len(X_train[0]), h1])), \n",
    "                  'bias':tf.Variable(tf.random_normal([h1]))} \n",
    " \n",
    "hidden_2 = {'weight':tf.Variable(tf.random_normal([h1, h2])), \n",
    "                  'bias':tf.Variable(tf.random_normal([h2]))} \n",
    "  \n",
    "output_layer = {'weight':tf.Variable(tf.random_normal([h2, n_classes])), \n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = tf.add(tf.matmul(x_input, hidden_1['weight']), hidden_1['bias']) \n",
    "l1 = tf.nn.relu(l1) \n",
    "\n",
    "l2 = tf.add(tf.matmul(l1, hidden_2['weight']), hidden_2['bias']) \n",
    "l2 = tf.nn.relu(l2) \n",
    "\n",
    "output = tf.matmul(l2, output_layer['weight']) + output_layer['bias'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_input)) \n",
    "opt = tf.train.AdamOptimizer().minimize(loss) \n",
    "\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        epoch_loss = 0 \n",
    "        i = 0 \n",
    "        while i < len(X_train): \n",
    "            start = i \n",
    "            end = i + batch_size \n",
    "            batch_x = np.array(X_train[start:end]) \n",
    "            batch_y = np.array(y_train[start:end]) \n",
    "\n",
    "            _, batch_loss = sess.run([opt, loss], feed_dict={x_input: batch_x, y_input: batch_y}) \n",
    "            epoch_loss += batch_loss\n",
    "            i += batch_size \n",
    "\n",
    "        print('Epoch {}: loss {}'.format(epoch, epoch_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
