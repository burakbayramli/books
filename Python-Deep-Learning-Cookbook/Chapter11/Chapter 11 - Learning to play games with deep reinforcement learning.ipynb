{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    # The ball is released after 2 frames\n",
    "    if i > 1:\n",
    "        print(observation.shape)\n",
    "        plt.imshow(observation)\n",
    "        plt.show()\n",
    "        \n",
    "    # Get the next observation\n",
    "    observation, _, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # remove top part of frame and some background\n",
    "    frame = frame[35:195, 10:150]\n",
    "    \n",
    "    # grayscale frame and downsize by factor 2\n",
    "    frame = frame[::2, ::2, 0]\n",
    "    \n",
    "    # set background to 0\n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    \n",
    "    # set ball and paddles to 1\n",
    "    frame[frame != 0] = 1\n",
    "    return frame.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_preprocessed = preprocess_frame(observation)\n",
    "plt.imshow(obs_preprocessed, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, cols, rows, n_actions, \n",
    "    batch_size=32):\n",
    "        self.state_size = (cols, rows, 4)\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - \n",
    "        self.epsilon_end) / self.exploration_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.n_actions)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * \n",
    "        K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        opt = Adam(lr=0.00025, epsilon=0.01)\n",
    "        updates = opt.get_updates\n",
    "        (self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y],\n",
    "        [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4),\n",
    "        activation='relu', input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), \n",
    "        activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1),\n",
    "        activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.n_actions))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def update_model(self):\n",
    "        self.target_model.set_weights\n",
    "        (self.model.get_weights())\n",
    "\n",
    "    def action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay(self, history, action, reward, \n",
    "               next_history, dead):\n",
    "        self.memory.append((history, action, \n",
    "        reward, next_history, dead))\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, \n",
    "        self.batch_size)\n",
    "        history = np.zeros((self.batch_size, \n",
    "        self.state_size[0], self.state_size[1], \n",
    "        self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size,\n",
    "        self.state_size[0], self.state_size[1], \n",
    "        self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32\n",
    "            (mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32\n",
    "            (mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.\n",
    "        predict(next_history)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + \n",
    "                self.discount_factor * \\\n",
    "                np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, \n",
    "        target])\n",
    "        self.avg_loss += loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "# General settings\n",
    "n_warmup_steps = 50000\n",
    "update_model_rate = 10000\n",
    "cols, rows = 85, 70\n",
    "n_states = 4\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "# Initialization\n",
    "agent = DQLAgent(cols, rows, n_actions=3)\n",
    "scores, episodes = [], []\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    done = False\n",
    "    dead = False\n",
    "    step, score, start_life = 0, 0, 5\n",
    "    observation = env.reset()\n",
    "\n",
    "\n",
    "    state = preprocess_frame(observation, \n",
    "    cols, rows)\n",
    "    history = np.stack((state, state, \n",
    "    state, state), axis=2)\n",
    "    history = np.reshape([history], \n",
    "    (1, cols, rows, n_states))\n",
    "\n",
    "    while not done:\n",
    "# env.render()\n",
    "        n_steps += 1\n",
    "        step += 1\n",
    "        \n",
    "        # Get action\n",
    "        action = agent.action(history)\n",
    "        observation, reward, done, info = \n",
    "        env.step(action+1)\n",
    "        \n",
    "        # Extract next state\n",
    "        state_next = preprocess_frame\n",
    "        (observation, cols, rows)\n",
    "        state_next = np.reshape([state_next], \n",
    "        (1, cols, rows, 1))\n",
    "        history_next = np.append(state_next,\n",
    "        history[:, :, :, :3], axis=3)\n",
    "\n",
    "        agent.avg_q_max += np.amax(agent.model\n",
    "        .predict(history)[0])\n",
    "        reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "        agent.replay(history, action, reward,\n",
    "        history_next, dead)\n",
    "        agent.train()\n",
    "        if n_steps % update_model_rate == 0:\n",
    "            agent.update_model()\n",
    "        score += reward\n",
    "\n",
    "        if dead:\n",
    "            dead = False\n",
    "        else:\n",
    "            history = history_next\n",
    "\n",
    "        if done:\n",
    "            print('episode {:2d}; score:\n",
    "            {:2.0f}; q {:2f}; loss {:2f}; steps {}'\n",
    "                  .format(n_steps, score, \n",
    "                  agent.avg_q_max / float(step),\n",
    "                  agent.avg_loss / float(step), step))\n",
    "\n",
    "            agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "    \n",
    "    # Save weights of model\n",
    "    if n_steps % 1000 == 0:\n",
    "        agent.model.save_weights\n",
    "        (\"weights/breakout_dql.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "agent = DQLAgent(cols, rows, n_action=3)\n",
    "\n",
    "for i in range(5):\n",
    "    observation = env.reset()\n",
    "\n",
    "    state = pre_processing(observation,\n",
    "    cols, rows)\n",
    "    history = np.stack((state, state, \n",
    "    state, state), axis=2)\n",
    "    history = np.reshape([history], (1, cols,\n",
    "    rows, n_states))\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.get_action(history)\n",
    "        observe, reward, done, info =\n",
    "        env.step(action+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
